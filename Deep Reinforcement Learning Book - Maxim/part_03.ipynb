{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f8b459",
   "metadata": {},
   "source": [
    "## Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07b8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import typing as tt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c15c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size: int, hidden_size: int, n_actions: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "@dataclass\n",
    "class Step:\n",
    "    observation: np.ndarray\n",
    "    action: int\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    reward: float\n",
    "    steps: tt.List[Step]\n",
    "\n",
    "def iterate_batches(env: gym.Env, net: Net, batch_size: int) -> tt.Generator[tt.List[Episode], None, None]:\n",
    "    batch = []\n",
    "    episode_reward = 0.\n",
    "    episode_steps = []\n",
    "    obs, _ = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "\n",
    "    while True:\n",
    "        obs_v = torch.tensor(obs, dtype=torch.float32)\n",
    "        act_probs_v = sm(net(obs_v.unsqueeze(0)))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, is_trunc, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = Step(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "\n",
    "        if is_done or is_trunc:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0\n",
    "            episode_steps = []\n",
    "            next_obs, _ = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "def filter_batch(batch: tt.List[Episode], percentile: float) -> tt.Tuple[torch.FloatTensor, torch.LongTensor, float, float]:\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = float(np.percentile(rewards, percentile))\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    train_obs: tt.List[np.ndarray] = []\n",
    "    train_act: tt.List[int] = []\n",
    "    \n",
    "    for episode in batch:\n",
    "        if episode.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, episode.steps))\n",
    "        train_act.extend(map(lambda step: step.action, episode.steps))\n",
    "    \n",
    "    train_obs_v = torch.FloatTensor(np.vstack(train_obs))\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ff15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.697, reward_mean=26.7, rw_bound=29.0\n",
      "1: loss=0.671, reward_mean=32.6, rw_bound=33.5\n",
      "2: loss=0.658, reward_mean=25.6, rw_bound=27.0\n",
      "3: loss=0.647, reward_mean=38.7, rw_bound=42.5\n",
      "4: loss=0.630, reward_mean=32.3, rw_bound=45.0\n",
      "5: loss=0.648, reward_mean=39.9, rw_bound=47.5\n",
      "6: loss=0.626, reward_mean=45.8, rw_bound=52.0\n",
      "7: loss=0.619, reward_mean=48.7, rw_bound=51.5\n",
      "8: loss=0.610, reward_mean=42.2, rw_bound=49.0\n",
      "9: loss=0.612, reward_mean=44.1, rw_bound=47.5\n",
      "10: loss=0.596, reward_mean=46.4, rw_bound=55.5\n",
      "11: loss=0.582, reward_mean=53.2, rw_bound=63.0\n",
      "12: loss=0.597, reward_mean=59.8, rw_bound=65.5\n",
      "13: loss=0.576, reward_mean=50.6, rw_bound=54.5\n",
      "14: loss=0.591, reward_mean=49.5, rw_bound=56.0\n",
      "15: loss=0.580, reward_mean=49.7, rw_bound=55.0\n",
      "16: loss=0.569, reward_mean=57.4, rw_bound=60.5\n",
      "17: loss=0.581, reward_mean=62.2, rw_bound=68.5\n",
      "18: loss=0.554, reward_mean=70.6, rw_bound=72.5\n",
      "19: loss=0.558, reward_mean=65.1, rw_bound=70.0\n",
      "20: loss=0.556, reward_mean=66.5, rw_bound=78.0\n",
      "21: loss=0.568, reward_mean=83.2, rw_bound=87.0\n",
      "22: loss=0.561, reward_mean=63.9, rw_bound=70.0\n",
      "23: loss=0.566, reward_mean=67.6, rw_bound=77.0\n",
      "24: loss=0.548, reward_mean=56.6, rw_bound=61.0\n",
      "25: loss=0.570, reward_mean=58.2, rw_bound=62.0\n",
      "26: loss=0.552, reward_mean=62.9, rw_bound=71.5\n",
      "27: loss=0.536, reward_mean=69.1, rw_bound=77.0\n",
      "28: loss=0.546, reward_mean=78.2, rw_bound=90.0\n",
      "29: loss=0.537, reward_mean=84.8, rw_bound=102.0\n",
      "30: loss=0.561, reward_mean=69.1, rw_bound=79.5\n",
      "31: loss=0.540, reward_mean=81.6, rw_bound=89.0\n",
      "32: loss=0.543, reward_mean=89.1, rw_bound=92.0\n",
      "33: loss=0.543, reward_mean=87.2, rw_bound=103.0\n",
      "34: loss=0.536, reward_mean=99.2, rw_bound=106.5\n",
      "35: loss=0.555, reward_mean=95.1, rw_bound=110.5\n",
      "36: loss=0.542, reward_mean=92.9, rw_bound=105.5\n",
      "37: loss=0.561, reward_mean=104.5, rw_bound=110.5\n",
      "38: loss=0.572, reward_mean=108.6, rw_bound=132.5\n",
      "39: loss=0.547, reward_mean=125.9, rw_bound=141.0\n",
      "40: loss=0.550, reward_mean=120.9, rw_bound=136.5\n",
      "41: loss=0.553, reward_mean=149.8, rw_bound=166.0\n",
      "42: loss=0.547, reward_mean=166.2, rw_bound=205.0\n",
      "43: loss=0.545, reward_mean=226.9, rw_bound=222.0\n",
      "44: loss=0.556, reward_mean=203.6, rw_bound=226.5\n",
      "45: loss=0.560, reward_mean=235.8, rw_bound=258.5\n",
      "46: loss=0.564, reward_mean=346.1, rw_bound=425.0\n",
      "47: loss=0.563, reward_mean=282.2, rw_bound=284.0\n",
      "48: loss=0.562, reward_mean=272.7, rw_bound=303.5\n",
      "49: loss=0.571, reward_mean=308.7, rw_bound=324.5\n",
      "50: loss=0.556, reward_mean=355.6, rw_bound=419.5\n",
      "51: loss=0.559, reward_mean=421.1, rw_bound=500.0\n",
      "52: loss=0.565, reward_mean=391.4, rw_bound=500.0\n",
      "53: loss=0.571, reward_mean=351.8, rw_bound=500.0\n",
      "54: loss=0.562, reward_mean=344.1, rw_bound=447.5\n",
      "55: loss=0.564, reward_mean=380.1, rw_bound=500.0\n",
      "56: loss=0.569, reward_mean=351.5, rw_bound=500.0\n",
      "57: loss=0.564, reward_mean=344.3, rw_bound=414.0\n",
      "58: loss=0.562, reward_mean=352.1, rw_bound=500.0\n",
      "59: loss=0.564, reward_mean=366.3, rw_bound=467.0\n",
      "60: loss=0.558, reward_mean=341.9, rw_bound=466.0\n",
      "61: loss=0.561, reward_mean=366.1, rw_bound=500.0\n",
      "62: loss=0.559, reward_mean=364.9, rw_bound=500.0\n",
      "63: loss=0.558, reward_mean=463.5, rw_bound=500.0\n",
      "64: loss=0.555, reward_mean=424.1, rw_bound=500.0\n",
      "65: loss=0.557, reward_mean=407.8, rw_bound=500.0\n",
      "66: loss=0.553, reward_mean=487.4, rw_bound=500.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    assert env.observation_space.shape is not None\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    n_actions = int(env.action_space.n)\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "\n",
    "        if reward_m > 475:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf2986",
   "metadata": {},
   "source": [
    "# Value Based Method\n",
    "\n",
    "### $$ V(s) = E [\\sum_{t=0}^\\infty \\gamma^t  R_t] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9a3b0",
   "metadata": {},
   "source": [
    "### Value Iteration Method for Frozen-Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d05912fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "State = int\n",
    "Action = int\n",
    "RewardKey = tt.Tuple[State, Action, State]\n",
    "TransitKey = tt.Tuple[State, Action]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)\n",
    "        self.transits: tt.Dict[TransitKey, Counter] = defaultdict(Counter)\n",
    "        self.values: tt.Dict[State, float] = defaultdict(float)\n",
    "    \n",
    "    def play_n_random_steps(self, n:int):\n",
    "        for _ in range(n):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "            rw_key = (self.state, action, new_state)\n",
    "            self.rewards[rw_key] = float(reward)\n",
    "            tr_key = (self.state, action)\n",
    "            self.transits[tr_key][new_state] += 1\n",
    "            if is_done or is_trunc:\n",
    "                self.state, _ = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "    def calc_action_value(self, state:State, action: Action) -> float:\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            rw_key = (state, action, tgt_state)\n",
    "            reward = self.rewards[rw_key]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "    \n",
    "    def select_action(self, state: State) -> Action:\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_action, best_value = action, action_value\n",
    "        return best_action\n",
    "    \n",
    "    def play_episode(self, env: gym.Env) -> float:\n",
    "        total_reward = 0.\n",
    "        state, _ = self.env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, is_trunc, _ = self.env.step(action)\n",
    "            rw_key = (state, action, new_state)\n",
    "            self.rewards[rw_key] = float(reward)\n",
    "            tr_key = (state, action)\n",
    "            self.transits[tr_key][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done or is_trunc:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            action_values = [self.calc_action_value(state, action) for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33168389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: Best reward updated 0.0 -> 0.1\n",
      "8: Best reward updated 0.1 -> 0.15\n",
      "9: Best reward updated 0.15 -> 0.5\n",
      "13: Best reward updated 0.5 -> 0.55\n",
      "14: Best reward updated 0.55 -> 0.6\n",
      "16: Best reward updated 0.6 -> 0.65\n",
      "18: Best reward updated 0.65 -> 0.7\n",
      "21: Best reward updated 0.7 -> 0.85\n",
      "Solved in 21 iterations!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "        reward = 0.\n",
    "        for _ in range(TEST_EPISODES): \n",
    "            reward += agent.play_episode(test_env) \n",
    "        reward /= TEST_EPISODES \n",
    "        writer.add_scalar(\"reward\", reward, iter_no) \n",
    "        if reward > best_reward: \n",
    "            print(f\"{iter_no}: Best reward updated {best_reward:.3} -> {reward:.3}\") \n",
    "            best_reward = reward \n",
    "        if reward > 0.80: \n",
    "            print(\"Solved in %d iterations!\" % iter_no) \n",
    "            break \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451548c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
