{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from scipy.spatial import cKDTree  # for fast nearest-neighbor queries\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "LATENT_DIM = 32\n",
    "HIDDEN_DIM = 64\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "MEMORY_CAPACITY = 10000\n",
    "EPISODES = 1000\n",
    "MAX_STEPS = 200\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "CERT_EPSILON = 0.1       # Allowed deviation in Q-value for certification\n",
    "MIN_RADIUS = 0.01\n",
    "MAX_RADIUS = 1.0\n",
    "CONTROLLER_THRESHOLD = 1.0  # TD error threshold for adjusting certification\n",
    "CONTROLLER_ALPHA = 0.9      # Factor to reduce radius if error is high\n",
    "CONTROLLER_BETA = 1.1       # Factor to increase radius if error is low\n",
    "CERT_BINARY_ITERS = 5       # Fixed iterations for binary search\n",
    "CERT_NUM_SAMPLES = 5        # Number of random perturbations per sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Encoder Network: Maps state to latent space.\n",
    "# ---------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=2, latent_dim=LATENT_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ---------------------------\n",
    "# QNetwork (Generalizer): Estimates Q-values from latent representation.\n",
    "# ---------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, num_actions=3):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "# ---------------------------\n",
    "# Replay Buffer for Experience Replay.\n",
    "# ---------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=MEMORY_CAPACITY):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), \n",
    "                np.array(rewards), np.array(next_states), \n",
    "                np.array(dones))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ---------------------------\n",
    "# Memory Module: Vectorized storage and fast KDâ€“Tree lookup.\n",
    "# ---------------------------\n",
    "class MemoryModule:\n",
    "    def __init__(self):\n",
    "        # We store latent vectors, radii, actions, and q_values in lists.\n",
    "        self.latents = []\n",
    "        self.radii = []\n",
    "        self.actions = []\n",
    "        self.q_values = []\n",
    "        self.kd_tree = None  # Will be built on demand\n",
    "\n",
    "    def add_batch(self, zs, actions, q_vals, radii):\n",
    "        # zs: (N, latent_dim) numpy array; actions, q_vals, radii: 1D arrays of length N.\n",
    "        self.latents.extend(zs)\n",
    "        self.actions.extend(actions)\n",
    "        self.q_values.extend(q_vals)\n",
    "        self.radii.extend(radii)\n",
    "        # Rebuild kd-tree after each batch addition\n",
    "        if len(self.latents) > 0:\n",
    "            self.kd_tree = cKDTree(np.array(self.latents))\n",
    "    \n",
    "    def get_certified_q(self, z):\n",
    "        # Given a latent vector z (1D numpy array), quickly find memory entries nearby.\n",
    "        if self.kd_tree is None:\n",
    "            return None\n",
    "        # Query all memory points within MAX_RADIUS (the maximum possible)\n",
    "        indices = self.kd_tree.query_ball_point(z, r=MAX_RADIUS)\n",
    "        if len(indices) == 0:\n",
    "            return None\n",
    "        certified_qs = []\n",
    "        for idx in indices:\n",
    "            mem_z = self.latents[idx]\n",
    "            # Check if z is within the certified radius for this memory entry.\n",
    "            if np.linalg.norm(z - mem_z) <= self.radii[idx]:\n",
    "                certified_qs.append(self.q_values[idx])\n",
    "        if len(certified_qs) > 0:\n",
    "            return np.mean(certified_qs)\n",
    "        return None\n",
    "    \n",
    "    def update_radii(self, td_error):\n",
    "        # Adjust all stored radii based on the average TD error.\n",
    "        new_radii = []\n",
    "        for r in self.radii:\n",
    "            if td_error > CONTROLLER_THRESHOLD:\n",
    "                new_r = max(MIN_RADIUS, r * CONTROLLER_ALPHA)\n",
    "            else:\n",
    "                new_r = min(MAX_RADIUS, r * CONTROLLER_BETA)\n",
    "            new_radii.append(new_r)\n",
    "        self.radii = new_radii\n",
    "\n",
    "# ---------------------------\n",
    "# Vectorized Certified Radius Computation.\n",
    "# ---------------------------\n",
    "def compute_certified_radii_batch(encoder, q_network, zs, actions, epsilon=CERT_EPSILON, \n",
    "                                  min_r=MIN_RADIUS, max_r=MAX_RADIUS,\n",
    "                                  num_samples=CERT_NUM_SAMPLES, binary_iters=CERT_BINARY_ITERS):\n",
    "    # zs: Tensor of shape (N, latent_dim)\n",
    "    # actions: Tensor of shape (N,)\n",
    "    N, d = zs.shape\n",
    "    # Initialize candidate radii (start at min_r for all)\n",
    "    radii = torch.full((N,), min_r, device=device)\n",
    "    low = torch.full((N,), min_r, device=device)\n",
    "    high = torch.full((N,), max_r, device=device)\n",
    "    \n",
    "    # For each binary search iteration, update candidate radii in vectorized fashion.\n",
    "    for _ in range(binary_iters):\n",
    "        mid = (low + high) / 2.0  # (N,)\n",
    "        # For each sample, generate num_samples random perturbations in latent space.\n",
    "        # Sample unit vectors: shape (N, num_samples, d)\n",
    "        deltas = torch.randn((N, num_samples, d), device=device)\n",
    "        deltas = deltas / (deltas.norm(dim=2, keepdim=True) + 1e-6)\n",
    "        # Sample random scales uniformly between 0 and mid.\n",
    "        scales = torch.rand((N, num_samples, 1), device=device) * mid.unsqueeze(1).unsqueeze(2)\n",
    "        perturbations = deltas * scales  # (N, num_samples, d)\n",
    "        zs_perturbed = zs.unsqueeze(1) + perturbations  # (N, num_samples, d)\n",
    "        # Flatten batch for evaluation.\n",
    "        zs_perturbed_flat = zs_perturbed.view(-1, d)\n",
    "        q_vals = q_network(zs_perturbed_flat)  # (N * num_samples, num_actions)\n",
    "        # Gather Q-values corresponding to actions.\n",
    "        actions_expanded = actions.unsqueeze(1).repeat(1, num_samples).view(-1)\n",
    "        q_vals = q_vals.gather(1, actions_expanded.unsqueeze(1)).view(N, num_samples)\n",
    "        # Compute original Q-values for each sample.\n",
    "        q_orig = q_network(zs).gather(1, actions.unsqueeze(1)).squeeze(1)  # (N,)\n",
    "        # Expand q_orig to shape (N, num_samples)\n",
    "        q_orig_expanded = q_orig.unsqueeze(1).expand_as(q_vals)\n",
    "        differences = torch.abs(q_vals - q_orig_expanded).max(dim=1)[0]  # (N,)\n",
    "        # If maximum difference <= epsilon, candidate radius is feasible.\n",
    "        feasible = differences <= epsilon\n",
    "        # Update low and high bounds vectorized:\n",
    "        low = torch.where(feasible, mid, low)\n",
    "        high = torch.where(feasible, high, mid)\n",
    "        radii = mid  # Use mid as the candidate radius.\n",
    "    return radii.detach().cpu().numpy()\n",
    "\n",
    "# ---------------------------\n",
    "# DQN Agent integrating Memory, Generalizer, and Controller.\n",
    "# ---------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.encoder = Encoder(input_dim=state_dim).to(device)\n",
    "        self.q_network = QNetwork(latent_dim=LATENT_DIM, num_actions=action_dim).to(device)\n",
    "        self.target_q_network = QNetwork(latent_dim=LATENT_DIM, num_actions=action_dim).to(device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(list(self.encoder.parameters()) + \n",
    "                                    list(self.q_network.parameters()), lr=LEARNING_RATE)\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.memory_module = MemoryModule()\n",
    "        self.epsilon = EPS_START\n",
    "        self.action_dim = action_dim\n",
    "        self.update_counter = 0  # Count mini-batch updates\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        # Cache latent representation.\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        z = self.encoder(state_tensor).detach().cpu().numpy().squeeze()\n",
    "        certified_q = self.memory_module.get_certified_q(z)\n",
    "        if certified_q is not None and np.random.rand() > self.epsilon:\n",
    "            # Use the certified memory Q-value indirectly: choose best action from Q-network.\n",
    "            with torch.no_grad():\n",
    "                q_vals = self.q_network(self.encoder(state_tensor))\n",
    "            action = q_vals.argmax().item()\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                action = np.random.randint(self.action_dim)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = self.q_network(self.encoder(state_tensor))\n",
    "                action = q_vals.argmax().item()\n",
    "        return action\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Encode current states\n",
    "        z_states = self.encoder(states)\n",
    "        q_values = self.q_network(z_states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values.\n",
    "        with torch.no_grad():\n",
    "            z_next = self.encoder(next_states)\n",
    "            next_q_values = self.target_q_network(z_next)\n",
    "            next_q_max, _ = next_q_values.max(dim=1)\n",
    "            target_q = rewards + GAMMA * next_q_max * (1 - dones)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.update_counter += 1\n",
    "        # Every few mini-batch updates, update the memory with certified data.\n",
    "        if self.update_counter % 10 == 0:\n",
    "            # Compute TD errors for the batch.\n",
    "            td_errors = torch.abs(q_values - target_q).detach().cpu().numpy()\n",
    "            avg_td_error = np.mean(td_errors)\n",
    "            # Compute certified radii for the entire batch in vectorized form.\n",
    "            radii = compute_certified_radii_batch(self.encoder, self.q_network, z_states, actions)\n",
    "            # Get q_values as numpy array.\n",
    "            q_vals_np = q_values.detach().cpu().numpy()\n",
    "            z_states_np = z_states.detach().cpu().numpy()\n",
    "            actions_np = actions.detach().cpu().numpy()\n",
    "            # Add this batch to the memory.\n",
    "            self.memory_module.add_batch(z_states_np, actions_np, q_vals_np, radii)\n",
    "            # Update memory radii using the Controller.\n",
    "            self.memory_module.update_radii(avg_td_error)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(EPS_END, self.epsilon * EPS_DECAY)\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "state_dim = env.observation_space.shape[0]  # e.g., 2: position and velocity.\n",
    "action_dim = env.action_space.n             # e.g., 3 actions.\n",
    "agent = DQNAgent(state_dim, action_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main Training Loop\n",
    "# ---------------------------\n",
    "def train():\n",
    "    scores = []\n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        for t in range(MAX_STEPS):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done or truncated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        agent.update()\n",
    "        agent.update_target_network()\n",
    "        agent.decay_epsilon()\n",
    "        scores.append(total_reward)\n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    env.close()\n",
    "    return scores\n",
    "\n",
    "# ---------------------------\n",
    "# Main Test Loop\n",
    "# ---------------------------\n",
    "def test():\n",
    "    scores = []\n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        for t in range(MAX_STEPS):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        scores.append(total_reward)\n",
    "        print(f\"Episode {episode+1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    env.close()\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -200.0, Epsilon: 0.995\n",
      "Episode 2, Total Reward: -200.0, Epsilon: 0.990\n",
      "Episode 3, Total Reward: -200.0, Epsilon: 0.985\n",
      "Episode 4, Total Reward: -200.0, Epsilon: 0.980\n",
      "Episode 5, Total Reward: -200.0, Epsilon: 0.975\n",
      "Episode 6, Total Reward: -200.0, Epsilon: 0.970\n",
      "Episode 7, Total Reward: -200.0, Epsilon: 0.966\n",
      "Episode 8, Total Reward: -200.0, Epsilon: 0.961\n",
      "Episode 9, Total Reward: -200.0, Epsilon: 0.956\n",
      "Episode 10, Total Reward: -200.0, Epsilon: 0.951\n",
      "Episode 11, Total Reward: -200.0, Epsilon: 0.946\n",
      "Episode 12, Total Reward: -200.0, Epsilon: 0.942\n",
      "Episode 13, Total Reward: -200.0, Epsilon: 0.937\n",
      "Episode 14, Total Reward: -200.0, Epsilon: 0.932\n",
      "Episode 15, Total Reward: -200.0, Epsilon: 0.928\n",
      "Episode 16, Total Reward: -200.0, Epsilon: 0.923\n",
      "Episode 17, Total Reward: -200.0, Epsilon: 0.918\n",
      "Episode 18, Total Reward: -200.0, Epsilon: 0.914\n",
      "Episode 19, Total Reward: -200.0, Epsilon: 0.909\n",
      "Episode 20, Total Reward: -200.0, Epsilon: 0.905\n",
      "Episode 21, Total Reward: -200.0, Epsilon: 0.900\n",
      "Episode 22, Total Reward: -200.0, Epsilon: 0.896\n",
      "Episode 23, Total Reward: -200.0, Epsilon: 0.891\n",
      "Episode 24, Total Reward: -200.0, Epsilon: 0.887\n",
      "Episode 25, Total Reward: -200.0, Epsilon: 0.882\n",
      "Episode 26, Total Reward: -200.0, Epsilon: 0.878\n",
      "Episode 27, Total Reward: -200.0, Epsilon: 0.873\n",
      "Episode 28, Total Reward: -200.0, Epsilon: 0.869\n",
      "Episode 29, Total Reward: -200.0, Epsilon: 0.865\n",
      "Episode 30, Total Reward: -200.0, Epsilon: 0.860\n",
      "Episode 31, Total Reward: -200.0, Epsilon: 0.856\n",
      "Episode 32, Total Reward: -200.0, Epsilon: 0.852\n",
      "Episode 33, Total Reward: -200.0, Epsilon: 0.848\n",
      "Episode 34, Total Reward: -200.0, Epsilon: 0.843\n",
      "Episode 35, Total Reward: -200.0, Epsilon: 0.839\n",
      "Episode 36, Total Reward: -200.0, Epsilon: 0.835\n",
      "Episode 37, Total Reward: -200.0, Epsilon: 0.831\n",
      "Episode 38, Total Reward: -200.0, Epsilon: 0.827\n",
      "Episode 39, Total Reward: -200.0, Epsilon: 0.822\n",
      "Episode 40, Total Reward: -200.0, Epsilon: 0.818\n",
      "Episode 41, Total Reward: -200.0, Epsilon: 0.814\n",
      "Episode 42, Total Reward: -200.0, Epsilon: 0.810\n",
      "Episode 43, Total Reward: -200.0, Epsilon: 0.806\n",
      "Episode 44, Total Reward: -200.0, Epsilon: 0.802\n",
      "Episode 45, Total Reward: -200.0, Epsilon: 0.798\n",
      "Episode 46, Total Reward: -200.0, Epsilon: 0.794\n",
      "Episode 47, Total Reward: -200.0, Epsilon: 0.790\n",
      "Episode 48, Total Reward: -200.0, Epsilon: 0.786\n",
      "Episode 49, Total Reward: -200.0, Epsilon: 0.782\n",
      "Episode 50, Total Reward: -200.0, Epsilon: 0.778\n",
      "Episode 51, Total Reward: -200.0, Epsilon: 0.774\n",
      "Episode 52, Total Reward: -200.0, Epsilon: 0.771\n",
      "Episode 53, Total Reward: -200.0, Epsilon: 0.767\n",
      "Episode 54, Total Reward: -200.0, Epsilon: 0.763\n",
      "Episode 55, Total Reward: -200.0, Epsilon: 0.759\n",
      "Episode 56, Total Reward: -200.0, Epsilon: 0.755\n",
      "Episode 57, Total Reward: -200.0, Epsilon: 0.751\n",
      "Episode 58, Total Reward: -200.0, Epsilon: 0.748\n",
      "Episode 59, Total Reward: -200.0, Epsilon: 0.744\n",
      "Episode 60, Total Reward: -200.0, Epsilon: 0.740\n",
      "Episode 61, Total Reward: -200.0, Epsilon: 0.737\n",
      "Episode 62, Total Reward: -200.0, Epsilon: 0.733\n",
      "Episode 63, Total Reward: -200.0, Epsilon: 0.729\n",
      "Episode 64, Total Reward: -200.0, Epsilon: 0.726\n",
      "Episode 65, Total Reward: -200.0, Epsilon: 0.722\n",
      "Episode 66, Total Reward: -200.0, Epsilon: 0.718\n",
      "Episode 67, Total Reward: -200.0, Epsilon: 0.715\n",
      "Episode 68, Total Reward: -200.0, Epsilon: 0.711\n",
      "Episode 69, Total Reward: -200.0, Epsilon: 0.708\n",
      "Episode 70, Total Reward: -200.0, Epsilon: 0.704\n",
      "Episode 71, Total Reward: -200.0, Epsilon: 0.701\n",
      "Episode 72, Total Reward: -200.0, Epsilon: 0.697\n",
      "Episode 73, Total Reward: -200.0, Epsilon: 0.694\n",
      "Episode 74, Total Reward: -200.0, Epsilon: 0.690\n",
      "Episode 75, Total Reward: -200.0, Epsilon: 0.687\n",
      "Episode 76, Total Reward: -200.0, Epsilon: 0.683\n",
      "Episode 77, Total Reward: -200.0, Epsilon: 0.680\n",
      "Episode 78, Total Reward: -200.0, Epsilon: 0.676\n",
      "Episode 79, Total Reward: -200.0, Epsilon: 0.673\n",
      "Episode 80, Total Reward: -200.0, Epsilon: 0.670\n",
      "Episode 81, Total Reward: -200.0, Epsilon: 0.666\n",
      "Episode 82, Total Reward: -200.0, Epsilon: 0.663\n",
      "Episode 83, Total Reward: -200.0, Epsilon: 0.660\n",
      "Episode 84, Total Reward: -200.0, Epsilon: 0.656\n",
      "Episode 85, Total Reward: -200.0, Epsilon: 0.653\n",
      "Episode 86, Total Reward: -200.0, Epsilon: 0.650\n",
      "Episode 87, Total Reward: -200.0, Epsilon: 0.647\n",
      "Episode 88, Total Reward: -200.0, Epsilon: 0.643\n",
      "Episode 89, Total Reward: -200.0, Epsilon: 0.640\n",
      "Episode 90, Total Reward: -200.0, Epsilon: 0.637\n",
      "Episode 91, Total Reward: -200.0, Epsilon: 0.634\n",
      "Episode 92, Total Reward: -200.0, Epsilon: 0.631\n",
      "Episode 93, Total Reward: -200.0, Epsilon: 0.627\n",
      "Episode 94, Total Reward: -200.0, Epsilon: 0.624\n",
      "Episode 95, Total Reward: -200.0, Epsilon: 0.621\n",
      "Episode 96, Total Reward: -200.0, Epsilon: 0.618\n",
      "Episode 97, Total Reward: -200.0, Epsilon: 0.615\n",
      "Episode 98, Total Reward: -200.0, Epsilon: 0.612\n",
      "Episode 99, Total Reward: -200.0, Epsilon: 0.609\n",
      "Episode 100, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 101, Total Reward: -200.0, Epsilon: 0.603\n",
      "Episode 102, Total Reward: -200.0, Epsilon: 0.600\n",
      "Episode 103, Total Reward: -200.0, Epsilon: 0.597\n",
      "Episode 104, Total Reward: -200.0, Epsilon: 0.594\n",
      "Episode 105, Total Reward: -200.0, Epsilon: 0.591\n",
      "Episode 106, Total Reward: -200.0, Epsilon: 0.588\n",
      "Episode 107, Total Reward: -200.0, Epsilon: 0.585\n",
      "Episode 108, Total Reward: -200.0, Epsilon: 0.582\n",
      "Episode 109, Total Reward: -200.0, Epsilon: 0.579\n",
      "Episode 110, Total Reward: -200.0, Epsilon: 0.576\n",
      "Episode 111, Total Reward: -200.0, Epsilon: 0.573\n",
      "Episode 112, Total Reward: -200.0, Epsilon: 0.570\n",
      "Episode 113, Total Reward: -200.0, Epsilon: 0.568\n",
      "Episode 114, Total Reward: -200.0, Epsilon: 0.565\n",
      "Episode 115, Total Reward: -200.0, Epsilon: 0.562\n",
      "Episode 116, Total Reward: -200.0, Epsilon: 0.559\n",
      "Episode 117, Total Reward: -200.0, Epsilon: 0.556\n",
      "Episode 118, Total Reward: -200.0, Epsilon: 0.554\n",
      "Episode 119, Total Reward: -200.0, Epsilon: 0.551\n",
      "Episode 120, Total Reward: -200.0, Epsilon: 0.548\n",
      "Episode 121, Total Reward: -200.0, Epsilon: 0.545\n",
      "Episode 122, Total Reward: -200.0, Epsilon: 0.543\n",
      "Episode 123, Total Reward: -200.0, Epsilon: 0.540\n",
      "Episode 124, Total Reward: -200.0, Epsilon: 0.537\n",
      "Episode 125, Total Reward: -200.0, Epsilon: 0.534\n",
      "Episode 126, Total Reward: -200.0, Epsilon: 0.532\n",
      "Episode 127, Total Reward: -200.0, Epsilon: 0.529\n",
      "Episode 128, Total Reward: -200.0, Epsilon: 0.526\n",
      "Episode 129, Total Reward: -200.0, Epsilon: 0.524\n",
      "Episode 130, Total Reward: -200.0, Epsilon: 0.521\n",
      "Episode 131, Total Reward: -200.0, Epsilon: 0.519\n",
      "Episode 132, Total Reward: -200.0, Epsilon: 0.516\n",
      "Episode 133, Total Reward: -200.0, Epsilon: 0.513\n",
      "Episode 134, Total Reward: -200.0, Epsilon: 0.511\n",
      "Episode 135, Total Reward: -200.0, Epsilon: 0.508\n",
      "Episode 136, Total Reward: -200.0, Epsilon: 0.506\n",
      "Episode 137, Total Reward: -200.0, Epsilon: 0.503\n",
      "Episode 138, Total Reward: -200.0, Epsilon: 0.501\n",
      "Episode 139, Total Reward: -200.0, Epsilon: 0.498\n",
      "Episode 140, Total Reward: -200.0, Epsilon: 0.496\n",
      "Episode 141, Total Reward: -200.0, Epsilon: 0.493\n",
      "Episode 142, Total Reward: -200.0, Epsilon: 0.491\n",
      "Episode 143, Total Reward: -200.0, Epsilon: 0.488\n",
      "Episode 144, Total Reward: -200.0, Epsilon: 0.486\n",
      "Episode 145, Total Reward: -200.0, Epsilon: 0.483\n",
      "Episode 146, Total Reward: -200.0, Epsilon: 0.481\n",
      "Episode 147, Total Reward: -200.0, Epsilon: 0.479\n",
      "Episode 148, Total Reward: -200.0, Epsilon: 0.476\n",
      "Episode 149, Total Reward: -200.0, Epsilon: 0.474\n",
      "Episode 150, Total Reward: -200.0, Epsilon: 0.471\n",
      "Episode 151, Total Reward: -200.0, Epsilon: 0.469\n",
      "Episode 152, Total Reward: -200.0, Epsilon: 0.467\n",
      "Episode 153, Total Reward: -200.0, Epsilon: 0.464\n",
      "Episode 154, Total Reward: -200.0, Epsilon: 0.462\n",
      "Episode 155, Total Reward: -200.0, Epsilon: 0.460\n",
      "Episode 156, Total Reward: -200.0, Epsilon: 0.458\n",
      "Episode 157, Total Reward: -200.0, Epsilon: 0.455\n",
      "Episode 158, Total Reward: -200.0, Epsilon: 0.453\n",
      "Episode 159, Total Reward: -200.0, Epsilon: 0.451\n",
      "Episode 160, Total Reward: -200.0, Epsilon: 0.448\n",
      "Episode 161, Total Reward: -200.0, Epsilon: 0.446\n",
      "Episode 162, Total Reward: -200.0, Epsilon: 0.444\n",
      "Episode 163, Total Reward: -200.0, Epsilon: 0.442\n",
      "Episode 164, Total Reward: -200.0, Epsilon: 0.440\n",
      "Episode 165, Total Reward: -200.0, Epsilon: 0.437\n",
      "Episode 166, Total Reward: -200.0, Epsilon: 0.435\n",
      "Episode 167, Total Reward: -200.0, Epsilon: 0.433\n",
      "Episode 168, Total Reward: -200.0, Epsilon: 0.431\n",
      "Episode 169, Total Reward: -200.0, Epsilon: 0.429\n",
      "Episode 170, Total Reward: -200.0, Epsilon: 0.427\n",
      "Episode 171, Total Reward: -200.0, Epsilon: 0.424\n",
      "Episode 172, Total Reward: -200.0, Epsilon: 0.422\n",
      "Episode 173, Total Reward: -200.0, Epsilon: 0.420\n",
      "Episode 174, Total Reward: -200.0, Epsilon: 0.418\n",
      "Episode 175, Total Reward: -200.0, Epsilon: 0.416\n",
      "Episode 176, Total Reward: -200.0, Epsilon: 0.414\n",
      "Episode 177, Total Reward: -200.0, Epsilon: 0.412\n",
      "Episode 178, Total Reward: -200.0, Epsilon: 0.410\n",
      "Episode 179, Total Reward: -200.0, Epsilon: 0.408\n",
      "Episode 180, Total Reward: -200.0, Epsilon: 0.406\n",
      "Episode 181, Total Reward: -200.0, Epsilon: 0.404\n",
      "Episode 182, Total Reward: -200.0, Epsilon: 0.402\n",
      "Episode 183, Total Reward: -200.0, Epsilon: 0.400\n",
      "Episode 184, Total Reward: -200.0, Epsilon: 0.398\n",
      "Episode 185, Total Reward: -200.0, Epsilon: 0.396\n",
      "Episode 186, Total Reward: -200.0, Epsilon: 0.394\n",
      "Episode 187, Total Reward: -200.0, Epsilon: 0.392\n",
      "Episode 188, Total Reward: -200.0, Epsilon: 0.390\n",
      "Episode 189, Total Reward: -200.0, Epsilon: 0.388\n",
      "Episode 190, Total Reward: -200.0, Epsilon: 0.386\n",
      "Episode 191, Total Reward: -200.0, Epsilon: 0.384\n",
      "Episode 192, Total Reward: -200.0, Epsilon: 0.382\n",
      "Episode 193, Total Reward: -200.0, Epsilon: 0.380\n",
      "Episode 194, Total Reward: -200.0, Epsilon: 0.378\n",
      "Episode 195, Total Reward: -200.0, Epsilon: 0.376\n",
      "Episode 196, Total Reward: -200.0, Epsilon: 0.374\n",
      "Episode 197, Total Reward: -200.0, Epsilon: 0.373\n",
      "Episode 198, Total Reward: -200.0, Epsilon: 0.371\n",
      "Episode 199, Total Reward: -200.0, Epsilon: 0.369\n",
      "Episode 200, Total Reward: -200.0, Epsilon: 0.367\n",
      "Episode 201, Total Reward: -200.0, Epsilon: 0.365\n",
      "Episode 202, Total Reward: -200.0, Epsilon: 0.363\n",
      "Episode 203, Total Reward: -200.0, Epsilon: 0.361\n",
      "Episode 204, Total Reward: -200.0, Epsilon: 0.360\n",
      "Episode 205, Total Reward: -200.0, Epsilon: 0.358\n",
      "Episode 206, Total Reward: -200.0, Epsilon: 0.356\n",
      "Episode 207, Total Reward: -200.0, Epsilon: 0.354\n",
      "Episode 208, Total Reward: -200.0, Epsilon: 0.353\n",
      "Episode 209, Total Reward: -200.0, Epsilon: 0.351\n",
      "Episode 210, Total Reward: -200.0, Epsilon: 0.349\n",
      "Episode 211, Total Reward: -200.0, Epsilon: 0.347\n",
      "Episode 212, Total Reward: -200.0, Epsilon: 0.346\n",
      "Episode 213, Total Reward: -200.0, Epsilon: 0.344\n",
      "Episode 214, Total Reward: -200.0, Epsilon: 0.342\n",
      "Episode 215, Total Reward: -200.0, Epsilon: 0.340\n",
      "Episode 216, Total Reward: -200.0, Epsilon: 0.339\n",
      "Episode 217, Total Reward: -200.0, Epsilon: 0.337\n",
      "Episode 218, Total Reward: -200.0, Epsilon: 0.335\n",
      "Episode 219, Total Reward: -200.0, Epsilon: 0.334\n",
      "Episode 220, Total Reward: -200.0, Epsilon: 0.332\n",
      "Episode 221, Total Reward: -200.0, Epsilon: 0.330\n",
      "Episode 222, Total Reward: -200.0, Epsilon: 0.329\n",
      "Episode 223, Total Reward: -200.0, Epsilon: 0.327\n",
      "Episode 224, Total Reward: -200.0, Epsilon: 0.325\n",
      "Episode 225, Total Reward: -200.0, Epsilon: 0.324\n",
      "Episode 226, Total Reward: -200.0, Epsilon: 0.322\n",
      "Episode 227, Total Reward: -200.0, Epsilon: 0.321\n",
      "Episode 228, Total Reward: -200.0, Epsilon: 0.319\n",
      "Episode 229, Total Reward: -200.0, Epsilon: 0.317\n",
      "Episode 230, Total Reward: -200.0, Epsilon: 0.316\n",
      "Episode 231, Total Reward: -200.0, Epsilon: 0.314\n",
      "Episode 232, Total Reward: -200.0, Epsilon: 0.313\n",
      "Episode 233, Total Reward: -200.0, Epsilon: 0.311\n",
      "Episode 234, Total Reward: -200.0, Epsilon: 0.309\n",
      "Episode 235, Total Reward: -200.0, Epsilon: 0.308\n",
      "Episode 236, Total Reward: -200.0, Epsilon: 0.306\n",
      "Episode 237, Total Reward: -200.0, Epsilon: 0.305\n",
      "Episode 238, Total Reward: -200.0, Epsilon: 0.303\n",
      "Episode 239, Total Reward: -200.0, Epsilon: 0.302\n",
      "Episode 240, Total Reward: -200.0, Epsilon: 0.300\n",
      "Episode 241, Total Reward: -200.0, Epsilon: 0.299\n",
      "Episode 242, Total Reward: -200.0, Epsilon: 0.297\n",
      "Episode 243, Total Reward: -200.0, Epsilon: 0.296\n",
      "Episode 244, Total Reward: -200.0, Epsilon: 0.294\n",
      "Episode 245, Total Reward: -200.0, Epsilon: 0.293\n",
      "Episode 246, Total Reward: -200.0, Epsilon: 0.291\n",
      "Episode 247, Total Reward: -200.0, Epsilon: 0.290\n",
      "Episode 248, Total Reward: -200.0, Epsilon: 0.288\n",
      "Episode 249, Total Reward: -200.0, Epsilon: 0.287\n",
      "Episode 250, Total Reward: -200.0, Epsilon: 0.286\n",
      "Episode 251, Total Reward: -200.0, Epsilon: 0.284\n",
      "Episode 252, Total Reward: -200.0, Epsilon: 0.283\n",
      "Episode 253, Total Reward: -200.0, Epsilon: 0.281\n",
      "Episode 254, Total Reward: -200.0, Epsilon: 0.280\n",
      "Episode 255, Total Reward: -200.0, Epsilon: 0.279\n",
      "Episode 256, Total Reward: -200.0, Epsilon: 0.277\n",
      "Episode 257, Total Reward: -200.0, Epsilon: 0.276\n",
      "Episode 258, Total Reward: -200.0, Epsilon: 0.274\n",
      "Episode 259, Total Reward: -200.0, Epsilon: 0.273\n",
      "Episode 260, Total Reward: -200.0, Epsilon: 0.272\n",
      "Episode 261, Total Reward: -200.0, Epsilon: 0.270\n",
      "Episode 262, Total Reward: -200.0, Epsilon: 0.269\n",
      "Episode 263, Total Reward: -200.0, Epsilon: 0.268\n",
      "Episode 264, Total Reward: -200.0, Epsilon: 0.266\n",
      "Episode 265, Total Reward: -200.0, Epsilon: 0.265\n",
      "Episode 266, Total Reward: -200.0, Epsilon: 0.264\n",
      "Episode 267, Total Reward: -200.0, Epsilon: 0.262\n",
      "Episode 268, Total Reward: -200.0, Epsilon: 0.261\n",
      "Episode 269, Total Reward: -200.0, Epsilon: 0.260\n",
      "Episode 270, Total Reward: -200.0, Epsilon: 0.258\n",
      "Episode 271, Total Reward: -200.0, Epsilon: 0.257\n",
      "Episode 272, Total Reward: -200.0, Epsilon: 0.256\n",
      "Episode 273, Total Reward: -200.0, Epsilon: 0.255\n",
      "Episode 274, Total Reward: -200.0, Epsilon: 0.253\n",
      "Episode 275, Total Reward: -200.0, Epsilon: 0.252\n",
      "Episode 276, Total Reward: -200.0, Epsilon: 0.251\n",
      "Episode 277, Total Reward: -200.0, Epsilon: 0.249\n",
      "Episode 278, Total Reward: -200.0, Epsilon: 0.248\n",
      "Episode 279, Total Reward: -200.0, Epsilon: 0.247\n",
      "Episode 280, Total Reward: -200.0, Epsilon: 0.246\n",
      "Episode 281, Total Reward: -200.0, Epsilon: 0.245\n",
      "Episode 282, Total Reward: -200.0, Epsilon: 0.243\n",
      "Episode 283, Total Reward: -200.0, Epsilon: 0.242\n",
      "Episode 284, Total Reward: -200.0, Epsilon: 0.241\n",
      "Episode 285, Total Reward: -200.0, Epsilon: 0.240\n",
      "Episode 286, Total Reward: -200.0, Epsilon: 0.238\n",
      "Episode 287, Total Reward: -200.0, Epsilon: 0.237\n",
      "Episode 288, Total Reward: -200.0, Epsilon: 0.236\n",
      "Episode 289, Total Reward: -200.0, Epsilon: 0.235\n",
      "Episode 290, Total Reward: -200.0, Epsilon: 0.234\n",
      "Episode 291, Total Reward: -200.0, Epsilon: 0.233\n",
      "Episode 292, Total Reward: -200.0, Epsilon: 0.231\n",
      "Episode 293, Total Reward: -200.0, Epsilon: 0.230\n",
      "Episode 294, Total Reward: -200.0, Epsilon: 0.229\n",
      "Episode 295, Total Reward: -200.0, Epsilon: 0.228\n",
      "Episode 296, Total Reward: -200.0, Epsilon: 0.227\n",
      "Episode 297, Total Reward: -200.0, Epsilon: 0.226\n",
      "Episode 298, Total Reward: -200.0, Epsilon: 0.225\n",
      "Episode 299, Total Reward: -200.0, Epsilon: 0.223\n",
      "Episode 300, Total Reward: -200.0, Epsilon: 0.222\n",
      "Episode 301, Total Reward: -200.0, Epsilon: 0.221\n",
      "Episode 302, Total Reward: -200.0, Epsilon: 0.220\n",
      "Episode 303, Total Reward: -200.0, Epsilon: 0.219\n",
      "Episode 304, Total Reward: -200.0, Epsilon: 0.218\n",
      "Episode 305, Total Reward: -200.0, Epsilon: 0.217\n",
      "Episode 306, Total Reward: -200.0, Epsilon: 0.216\n",
      "Episode 307, Total Reward: -200.0, Epsilon: 0.215\n",
      "Episode 308, Total Reward: -200.0, Epsilon: 0.214\n",
      "Episode 309, Total Reward: -200.0, Epsilon: 0.212\n",
      "Episode 310, Total Reward: -200.0, Epsilon: 0.211\n",
      "Episode 311, Total Reward: -200.0, Epsilon: 0.210\n",
      "Episode 312, Total Reward: -200.0, Epsilon: 0.209\n",
      "Episode 313, Total Reward: -200.0, Epsilon: 0.208\n",
      "Episode 314, Total Reward: -200.0, Epsilon: 0.207\n",
      "Episode 315, Total Reward: -200.0, Epsilon: 0.206\n",
      "Episode 316, Total Reward: -200.0, Epsilon: 0.205\n",
      "Episode 317, Total Reward: -200.0, Epsilon: 0.204\n",
      "Episode 318, Total Reward: -200.0, Epsilon: 0.203\n",
      "Episode 319, Total Reward: -200.0, Epsilon: 0.202\n",
      "Episode 320, Total Reward: -200.0, Epsilon: 0.201\n",
      "Episode 321, Total Reward: -200.0, Epsilon: 0.200\n",
      "Episode 322, Total Reward: -200.0, Epsilon: 0.199\n",
      "Episode 323, Total Reward: -200.0, Epsilon: 0.198\n",
      "Episode 324, Total Reward: -200.0, Epsilon: 0.197\n",
      "Episode 325, Total Reward: -200.0, Epsilon: 0.196\n",
      "Episode 326, Total Reward: -200.0, Epsilon: 0.195\n",
      "Episode 327, Total Reward: -200.0, Epsilon: 0.194\n",
      "Episode 328, Total Reward: -200.0, Epsilon: 0.193\n",
      "Episode 329, Total Reward: -200.0, Epsilon: 0.192\n",
      "Episode 330, Total Reward: -200.0, Epsilon: 0.191\n",
      "Episode 331, Total Reward: -200.0, Epsilon: 0.190\n",
      "Episode 332, Total Reward: -200.0, Epsilon: 0.189\n",
      "Episode 333, Total Reward: -200.0, Epsilon: 0.188\n",
      "Episode 334, Total Reward: -200.0, Epsilon: 0.187\n",
      "Episode 335, Total Reward: -200.0, Epsilon: 0.187\n",
      "Episode 336, Total Reward: -200.0, Epsilon: 0.186\n",
      "Episode 337, Total Reward: -200.0, Epsilon: 0.185\n",
      "Episode 338, Total Reward: -200.0, Epsilon: 0.184\n",
      "Episode 339, Total Reward: -200.0, Epsilon: 0.183\n",
      "Episode 340, Total Reward: -200.0, Epsilon: 0.182\n",
      "Episode 341, Total Reward: -200.0, Epsilon: 0.181\n",
      "Episode 342, Total Reward: -200.0, Epsilon: 0.180\n",
      "Episode 343, Total Reward: -200.0, Epsilon: 0.179\n",
      "Episode 344, Total Reward: -200.0, Epsilon: 0.178\n",
      "Episode 345, Total Reward: -200.0, Epsilon: 0.177\n",
      "Episode 346, Total Reward: -200.0, Epsilon: 0.177\n",
      "Episode 347, Total Reward: -200.0, Epsilon: 0.176\n",
      "Episode 348, Total Reward: -200.0, Epsilon: 0.175\n",
      "Episode 349, Total Reward: -200.0, Epsilon: 0.174\n",
      "Episode 350, Total Reward: -200.0, Epsilon: 0.173\n",
      "Episode 351, Total Reward: -200.0, Epsilon: 0.172\n",
      "Episode 352, Total Reward: -200.0, Epsilon: 0.171\n",
      "Episode 353, Total Reward: -200.0, Epsilon: 0.170\n",
      "Episode 354, Total Reward: -200.0, Epsilon: 0.170\n",
      "Episode 355, Total Reward: -200.0, Epsilon: 0.169\n",
      "Episode 356, Total Reward: -200.0, Epsilon: 0.168\n",
      "Episode 357, Total Reward: -200.0, Epsilon: 0.167\n",
      "Episode 358, Total Reward: -200.0, Epsilon: 0.166\n",
      "Episode 359, Total Reward: -200.0, Epsilon: 0.165\n",
      "Episode 360, Total Reward: -200.0, Epsilon: 0.165\n",
      "Episode 361, Total Reward: -200.0, Epsilon: 0.164\n",
      "Episode 362, Total Reward: -200.0, Epsilon: 0.163\n",
      "Episode 363, Total Reward: -200.0, Epsilon: 0.162\n",
      "Episode 364, Total Reward: -200.0, Epsilon: 0.161\n",
      "Episode 365, Total Reward: -200.0, Epsilon: 0.160\n",
      "Episode 366, Total Reward: -200.0, Epsilon: 0.160\n",
      "Episode 367, Total Reward: -200.0, Epsilon: 0.159\n",
      "Episode 368, Total Reward: -200.0, Epsilon: 0.158\n",
      "Episode 369, Total Reward: -200.0, Epsilon: 0.157\n",
      "Episode 370, Total Reward: -200.0, Epsilon: 0.157\n",
      "Episode 371, Total Reward: -200.0, Epsilon: 0.156\n",
      "Episode 372, Total Reward: -200.0, Epsilon: 0.155\n",
      "Episode 373, Total Reward: -200.0, Epsilon: 0.154\n",
      "Episode 374, Total Reward: -200.0, Epsilon: 0.153\n",
      "Episode 375, Total Reward: -200.0, Epsilon: 0.153\n",
      "Episode 376, Total Reward: -200.0, Epsilon: 0.152\n",
      "Episode 377, Total Reward: -200.0, Epsilon: 0.151\n",
      "Episode 378, Total Reward: -200.0, Epsilon: 0.150\n",
      "Episode 379, Total Reward: -200.0, Epsilon: 0.150\n",
      "Episode 380, Total Reward: -200.0, Epsilon: 0.149\n",
      "Episode 381, Total Reward: -200.0, Epsilon: 0.148\n",
      "Episode 382, Total Reward: -200.0, Epsilon: 0.147\n",
      "Episode 383, Total Reward: -200.0, Epsilon: 0.147\n",
      "Episode 384, Total Reward: -200.0, Epsilon: 0.146\n",
      "Episode 385, Total Reward: -200.0, Epsilon: 0.145\n",
      "Episode 386, Total Reward: -200.0, Epsilon: 0.144\n",
      "Episode 387, Total Reward: -200.0, Epsilon: 0.144\n",
      "Episode 388, Total Reward: -200.0, Epsilon: 0.143\n",
      "Episode 389, Total Reward: -200.0, Epsilon: 0.142\n",
      "Episode 390, Total Reward: -200.0, Epsilon: 0.142\n",
      "Episode 391, Total Reward: -200.0, Epsilon: 0.141\n",
      "Episode 392, Total Reward: -200.0, Epsilon: 0.140\n",
      "Episode 393, Total Reward: -200.0, Epsilon: 0.139\n",
      "Episode 394, Total Reward: -200.0, Epsilon: 0.139\n",
      "Episode 395, Total Reward: -200.0, Epsilon: 0.138\n",
      "Episode 396, Total Reward: -200.0, Epsilon: 0.137\n",
      "Episode 397, Total Reward: -200.0, Epsilon: 0.137\n",
      "Episode 398, Total Reward: -200.0, Epsilon: 0.136\n",
      "Episode 399, Total Reward: -200.0, Epsilon: 0.135\n",
      "Episode 400, Total Reward: -200.0, Epsilon: 0.135\n",
      "Episode 401, Total Reward: -200.0, Epsilon: 0.134\n",
      "Episode 402, Total Reward: -200.0, Epsilon: 0.133\n",
      "Episode 403, Total Reward: -200.0, Epsilon: 0.133\n",
      "Episode 404, Total Reward: -200.0, Epsilon: 0.132\n",
      "Episode 405, Total Reward: -200.0, Epsilon: 0.131\n",
      "Episode 406, Total Reward: -200.0, Epsilon: 0.131\n",
      "Episode 407, Total Reward: -200.0, Epsilon: 0.130\n",
      "Episode 408, Total Reward: -200.0, Epsilon: 0.129\n",
      "Episode 409, Total Reward: -200.0, Epsilon: 0.129\n",
      "Episode 410, Total Reward: -200.0, Epsilon: 0.128\n",
      "Episode 411, Total Reward: -200.0, Epsilon: 0.127\n",
      "Episode 412, Total Reward: -200.0, Epsilon: 0.127\n",
      "Episode 413, Total Reward: -200.0, Epsilon: 0.126\n",
      "Episode 414, Total Reward: -200.0, Epsilon: 0.126\n",
      "Episode 415, Total Reward: -200.0, Epsilon: 0.125\n",
      "Episode 416, Total Reward: -200.0, Epsilon: 0.124\n",
      "Episode 417, Total Reward: -200.0, Epsilon: 0.124\n",
      "Episode 418, Total Reward: -200.0, Epsilon: 0.123\n",
      "Episode 419, Total Reward: -200.0, Epsilon: 0.122\n",
      "Episode 420, Total Reward: -200.0, Epsilon: 0.122\n",
      "Episode 421, Total Reward: -200.0, Epsilon: 0.121\n",
      "Episode 422, Total Reward: -200.0, Epsilon: 0.121\n",
      "Episode 423, Total Reward: -200.0, Epsilon: 0.120\n",
      "Episode 424, Total Reward: -200.0, Epsilon: 0.119\n",
      "Episode 425, Total Reward: -200.0, Epsilon: 0.119\n",
      "Episode 426, Total Reward: -200.0, Epsilon: 0.118\n",
      "Episode 427, Total Reward: -200.0, Epsilon: 0.118\n",
      "Episode 428, Total Reward: -200.0, Epsilon: 0.117\n",
      "Episode 429, Total Reward: -200.0, Epsilon: 0.116\n",
      "Episode 430, Total Reward: -200.0, Epsilon: 0.116\n",
      "Episode 431, Total Reward: -200.0, Epsilon: 0.115\n",
      "Episode 432, Total Reward: -200.0, Epsilon: 0.115\n",
      "Episode 433, Total Reward: -200.0, Epsilon: 0.114\n",
      "Episode 434, Total Reward: -200.0, Epsilon: 0.114\n",
      "Episode 435, Total Reward: -200.0, Epsilon: 0.113\n",
      "Episode 436, Total Reward: -200.0, Epsilon: 0.112\n",
      "Episode 437, Total Reward: -200.0, Epsilon: 0.112\n",
      "Episode 438, Total Reward: -200.0, Epsilon: 0.111\n",
      "Episode 439, Total Reward: -200.0, Epsilon: 0.111\n",
      "Episode 440, Total Reward: -200.0, Epsilon: 0.110\n",
      "Episode 441, Total Reward: -200.0, Epsilon: 0.110\n",
      "Episode 442, Total Reward: -200.0, Epsilon: 0.109\n",
      "Episode 443, Total Reward: -200.0, Epsilon: 0.109\n",
      "Episode 444, Total Reward: -200.0, Epsilon: 0.108\n",
      "Episode 445, Total Reward: -200.0, Epsilon: 0.107\n",
      "Episode 446, Total Reward: -200.0, Epsilon: 0.107\n",
      "Episode 447, Total Reward: -200.0, Epsilon: 0.106\n",
      "Episode 448, Total Reward: -200.0, Epsilon: 0.106\n",
      "Episode 449, Total Reward: -200.0, Epsilon: 0.105\n",
      "Episode 450, Total Reward: -200.0, Epsilon: 0.105\n",
      "Episode 451, Total Reward: -200.0, Epsilon: 0.104\n",
      "Episode 452, Total Reward: -200.0, Epsilon: 0.104\n",
      "Episode 453, Total Reward: -200.0, Epsilon: 0.103\n",
      "Episode 454, Total Reward: -200.0, Epsilon: 0.103\n",
      "Episode 455, Total Reward: -200.0, Epsilon: 0.102\n",
      "Episode 456, Total Reward: -200.0, Epsilon: 0.102\n",
      "Episode 457, Total Reward: -200.0, Epsilon: 0.101\n",
      "Episode 458, Total Reward: -200.0, Epsilon: 0.101\n",
      "Episode 459, Total Reward: -200.0, Epsilon: 0.100\n",
      "Episode 460, Total Reward: -200.0, Epsilon: 0.100\n",
      "Episode 461, Total Reward: -200.0, Epsilon: 0.099\n",
      "Episode 462, Total Reward: -200.0, Epsilon: 0.099\n",
      "Episode 463, Total Reward: -200.0, Epsilon: 0.098\n",
      "Episode 464, Total Reward: -200.0, Epsilon: 0.098\n",
      "Episode 465, Total Reward: -200.0, Epsilon: 0.097\n",
      "Episode 466, Total Reward: -200.0, Epsilon: 0.097\n",
      "Episode 467, Total Reward: -200.0, Epsilon: 0.096\n",
      "Episode 468, Total Reward: -200.0, Epsilon: 0.096\n",
      "Episode 469, Total Reward: -200.0, Epsilon: 0.095\n",
      "Episode 470, Total Reward: -200.0, Epsilon: 0.095\n",
      "Episode 471, Total Reward: -200.0, Epsilon: 0.094\n",
      "Episode 472, Total Reward: -200.0, Epsilon: 0.094\n",
      "Episode 473, Total Reward: -200.0, Epsilon: 0.093\n",
      "Episode 474, Total Reward: -200.0, Epsilon: 0.093\n",
      "Episode 475, Total Reward: -200.0, Epsilon: 0.092\n",
      "Episode 476, Total Reward: -200.0, Epsilon: 0.092\n",
      "Episode 477, Total Reward: -200.0, Epsilon: 0.092\n",
      "Episode 478, Total Reward: -200.0, Epsilon: 0.091\n",
      "Episode 479, Total Reward: -200.0, Epsilon: 0.091\n",
      "Episode 480, Total Reward: -200.0, Epsilon: 0.090\n",
      "Episode 481, Total Reward: -200.0, Epsilon: 0.090\n",
      "Episode 482, Total Reward: -200.0, Epsilon: 0.089\n",
      "Episode 483, Total Reward: -200.0, Epsilon: 0.089\n",
      "Episode 484, Total Reward: -200.0, Epsilon: 0.088\n",
      "Episode 485, Total Reward: -200.0, Epsilon: 0.088\n",
      "Episode 486, Total Reward: -200.0, Epsilon: 0.088\n",
      "Episode 487, Total Reward: -200.0, Epsilon: 0.087\n",
      "Episode 488, Total Reward: -200.0, Epsilon: 0.087\n",
      "Episode 489, Total Reward: -200.0, Epsilon: 0.086\n",
      "Episode 490, Total Reward: -200.0, Epsilon: 0.086\n",
      "Episode 491, Total Reward: -200.0, Epsilon: 0.085\n",
      "Episode 492, Total Reward: -200.0, Epsilon: 0.085\n",
      "Episode 493, Total Reward: -200.0, Epsilon: 0.084\n",
      "Episode 494, Total Reward: -200.0, Epsilon: 0.084\n",
      "Episode 495, Total Reward: -200.0, Epsilon: 0.084\n",
      "Episode 496, Total Reward: -200.0, Epsilon: 0.083\n",
      "Episode 497, Total Reward: -200.0, Epsilon: 0.083\n",
      "Episode 498, Total Reward: -200.0, Epsilon: 0.082\n",
      "Episode 499, Total Reward: -200.0, Epsilon: 0.082\n",
      "Episode 500, Total Reward: -200.0, Epsilon: 0.082\n",
      "Episode 501, Total Reward: -200.0, Epsilon: 0.081\n",
      "Episode 502, Total Reward: -200.0, Epsilon: 0.081\n",
      "Episode 503, Total Reward: -200.0, Epsilon: 0.080\n",
      "Episode 504, Total Reward: -200.0, Epsilon: 0.080\n",
      "Episode 505, Total Reward: -200.0, Epsilon: 0.080\n",
      "Episode 506, Total Reward: -200.0, Epsilon: 0.079\n",
      "Episode 507, Total Reward: -200.0, Epsilon: 0.079\n",
      "Episode 508, Total Reward: -200.0, Epsilon: 0.078\n",
      "Episode 509, Total Reward: -200.0, Epsilon: 0.078\n",
      "Episode 510, Total Reward: -200.0, Epsilon: 0.078\n",
      "Episode 511, Total Reward: -200.0, Epsilon: 0.077\n",
      "Episode 512, Total Reward: -200.0, Epsilon: 0.077\n",
      "Episode 513, Total Reward: -200.0, Epsilon: 0.076\n",
      "Episode 514, Total Reward: -200.0, Epsilon: 0.076\n",
      "Episode 515, Total Reward: -200.0, Epsilon: 0.076\n",
      "Episode 516, Total Reward: -200.0, Epsilon: 0.075\n",
      "Episode 517, Total Reward: -200.0, Epsilon: 0.075\n",
      "Episode 518, Total Reward: -200.0, Epsilon: 0.075\n",
      "Episode 519, Total Reward: -200.0, Epsilon: 0.074\n",
      "Episode 520, Total Reward: -200.0, Epsilon: 0.074\n",
      "Episode 521, Total Reward: -200.0, Epsilon: 0.073\n",
      "Episode 522, Total Reward: -200.0, Epsilon: 0.073\n",
      "Episode 523, Total Reward: -200.0, Epsilon: 0.073\n",
      "Episode 524, Total Reward: -200.0, Epsilon: 0.072\n",
      "Episode 525, Total Reward: -200.0, Epsilon: 0.072\n",
      "Episode 526, Total Reward: -200.0, Epsilon: 0.072\n",
      "Episode 527, Total Reward: -200.0, Epsilon: 0.071\n",
      "Episode 528, Total Reward: -200.0, Epsilon: 0.071\n",
      "Episode 529, Total Reward: -200.0, Epsilon: 0.071\n",
      "Episode 530, Total Reward: -200.0, Epsilon: 0.070\n",
      "Episode 531, Total Reward: -200.0, Epsilon: 0.070\n",
      "Episode 532, Total Reward: -200.0, Epsilon: 0.069\n",
      "Episode 533, Total Reward: -200.0, Epsilon: 0.069\n",
      "Episode 534, Total Reward: -200.0, Epsilon: 0.069\n",
      "Episode 535, Total Reward: -200.0, Epsilon: 0.068\n",
      "Episode 536, Total Reward: -200.0, Epsilon: 0.068\n",
      "Episode 537, Total Reward: -200.0, Epsilon: 0.068\n",
      "Episode 538, Total Reward: -200.0, Epsilon: 0.067\n",
      "Episode 539, Total Reward: -200.0, Epsilon: 0.067\n",
      "Episode 540, Total Reward: -200.0, Epsilon: 0.067\n",
      "Episode 541, Total Reward: -200.0, Epsilon: 0.066\n",
      "Episode 542, Total Reward: -200.0, Epsilon: 0.066\n",
      "Episode 543, Total Reward: -200.0, Epsilon: 0.066\n",
      "Episode 544, Total Reward: -200.0, Epsilon: 0.065\n",
      "Episode 545, Total Reward: -200.0, Epsilon: 0.065\n",
      "Episode 546, Total Reward: -200.0, Epsilon: 0.065\n",
      "Episode 547, Total Reward: -200.0, Epsilon: 0.064\n",
      "Episode 548, Total Reward: -200.0, Epsilon: 0.064\n",
      "Episode 549, Total Reward: -200.0, Epsilon: 0.064\n",
      "Episode 550, Total Reward: -200.0, Epsilon: 0.063\n",
      "Episode 551, Total Reward: -200.0, Epsilon: 0.063\n",
      "Episode 552, Total Reward: -200.0, Epsilon: 0.063\n",
      "Episode 553, Total Reward: -200.0, Epsilon: 0.063\n",
      "Episode 554, Total Reward: -200.0, Epsilon: 0.062\n",
      "Episode 555, Total Reward: -200.0, Epsilon: 0.062\n",
      "Episode 556, Total Reward: -200.0, Epsilon: 0.062\n",
      "Episode 557, Total Reward: -200.0, Epsilon: 0.061\n",
      "Episode 558, Total Reward: -200.0, Epsilon: 0.061\n",
      "Episode 559, Total Reward: -200.0, Epsilon: 0.061\n",
      "Episode 560, Total Reward: -200.0, Epsilon: 0.060\n",
      "Episode 561, Total Reward: -200.0, Epsilon: 0.060\n",
      "Episode 562, Total Reward: -200.0, Epsilon: 0.060\n",
      "Episode 563, Total Reward: -200.0, Epsilon: 0.059\n",
      "Episode 564, Total Reward: -200.0, Epsilon: 0.059\n",
      "Episode 565, Total Reward: -200.0, Epsilon: 0.059\n",
      "Episode 566, Total Reward: -200.0, Epsilon: 0.059\n",
      "Episode 567, Total Reward: -200.0, Epsilon: 0.058\n",
      "Episode 568, Total Reward: -200.0, Epsilon: 0.058\n",
      "Episode 569, Total Reward: -200.0, Epsilon: 0.058\n",
      "Episode 570, Total Reward: -200.0, Epsilon: 0.057\n",
      "Episode 571, Total Reward: -200.0, Epsilon: 0.057\n",
      "Episode 572, Total Reward: -200.0, Epsilon: 0.057\n",
      "Episode 573, Total Reward: -200.0, Epsilon: 0.057\n",
      "Episode 574, Total Reward: -200.0, Epsilon: 0.056\n",
      "Episode 575, Total Reward: -200.0, Epsilon: 0.056\n",
      "Episode 576, Total Reward: -200.0, Epsilon: 0.056\n",
      "Episode 577, Total Reward: -200.0, Epsilon: 0.055\n",
      "Episode 578, Total Reward: -200.0, Epsilon: 0.055\n",
      "Episode 579, Total Reward: -200.0, Epsilon: 0.055\n",
      "Episode 580, Total Reward: -200.0, Epsilon: 0.055\n",
      "Episode 581, Total Reward: -200.0, Epsilon: 0.054\n",
      "Episode 582, Total Reward: -200.0, Epsilon: 0.054\n",
      "Episode 583, Total Reward: -200.0, Epsilon: 0.054\n",
      "Episode 584, Total Reward: -200.0, Epsilon: 0.054\n",
      "Episode 585, Total Reward: -200.0, Epsilon: 0.053\n",
      "Episode 586, Total Reward: -200.0, Epsilon: 0.053\n",
      "Episode 587, Total Reward: -200.0, Epsilon: 0.053\n",
      "Episode 588, Total Reward: -200.0, Epsilon: 0.052\n",
      "Episode 589, Total Reward: -200.0, Epsilon: 0.052\n",
      "Episode 590, Total Reward: -200.0, Epsilon: 0.052\n",
      "Episode 591, Total Reward: -200.0, Epsilon: 0.052\n",
      "Episode 592, Total Reward: -200.0, Epsilon: 0.051\n",
      "Episode 593, Total Reward: -200.0, Epsilon: 0.051\n",
      "Episode 594, Total Reward: -200.0, Epsilon: 0.051\n",
      "Episode 595, Total Reward: -200.0, Epsilon: 0.051\n",
      "Episode 596, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 597, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 598, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 599, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 600, Total Reward: -200.0, Epsilon: 0.049\n",
      "Episode 601, Total Reward: -200.0, Epsilon: 0.049\n",
      "Episode 602, Total Reward: -200.0, Epsilon: 0.049\n",
      "Episode 603, Total Reward: -200.0, Epsilon: 0.049\n",
      "Episode 604, Total Reward: -200.0, Epsilon: 0.048\n",
      "Episode 605, Total Reward: -200.0, Epsilon: 0.048\n",
      "Episode 606, Total Reward: -200.0, Epsilon: 0.048\n",
      "Episode 607, Total Reward: -200.0, Epsilon: 0.048\n",
      "Episode 608, Total Reward: -200.0, Epsilon: 0.047\n",
      "Episode 609, Total Reward: -200.0, Epsilon: 0.047\n",
      "Episode 610, Total Reward: -200.0, Epsilon: 0.047\n",
      "Episode 611, Total Reward: -200.0, Epsilon: 0.047\n",
      "Episode 612, Total Reward: -200.0, Epsilon: 0.047\n",
      "Episode 613, Total Reward: -200.0, Epsilon: 0.046\n",
      "Episode 614, Total Reward: -200.0, Epsilon: 0.046\n",
      "Episode 615, Total Reward: -200.0, Epsilon: 0.046\n",
      "Episode 616, Total Reward: -200.0, Epsilon: 0.046\n",
      "Episode 617, Total Reward: -200.0, Epsilon: 0.045\n",
      "Episode 618, Total Reward: -200.0, Epsilon: 0.045\n",
      "Episode 619, Total Reward: -200.0, Epsilon: 0.045\n",
      "Episode 620, Total Reward: -200.0, Epsilon: 0.045\n",
      "Episode 621, Total Reward: -200.0, Epsilon: 0.044\n",
      "Episode 622, Total Reward: -200.0, Epsilon: 0.044\n",
      "Episode 623, Total Reward: -200.0, Epsilon: 0.044\n",
      "Episode 624, Total Reward: -200.0, Epsilon: 0.044\n",
      "Episode 625, Total Reward: -200.0, Epsilon: 0.044\n",
      "Episode 626, Total Reward: -200.0, Epsilon: 0.043\n",
      "Episode 627, Total Reward: -200.0, Epsilon: 0.043\n",
      "Episode 628, Total Reward: -200.0, Epsilon: 0.043\n",
      "Episode 629, Total Reward: -200.0, Epsilon: 0.043\n",
      "Episode 630, Total Reward: -200.0, Epsilon: 0.043\n",
      "Episode 631, Total Reward: -200.0, Epsilon: 0.042\n",
      "Episode 632, Total Reward: -200.0, Epsilon: 0.042\n",
      "Episode 633, Total Reward: -200.0, Epsilon: 0.042\n",
      "Episode 634, Total Reward: -200.0, Epsilon: 0.042\n",
      "Episode 635, Total Reward: -200.0, Epsilon: 0.041\n",
      "Episode 636, Total Reward: -200.0, Epsilon: 0.041\n",
      "Episode 637, Total Reward: -200.0, Epsilon: 0.041\n",
      "Episode 638, Total Reward: -200.0, Epsilon: 0.041\n",
      "Episode 639, Total Reward: -200.0, Epsilon: 0.041\n",
      "Episode 640, Total Reward: -200.0, Epsilon: 0.040\n",
      "Episode 641, Total Reward: -200.0, Epsilon: 0.040\n",
      "Episode 642, Total Reward: -200.0, Epsilon: 0.040\n",
      "Episode 643, Total Reward: -200.0, Epsilon: 0.040\n",
      "Episode 644, Total Reward: -200.0, Epsilon: 0.040\n",
      "Episode 645, Total Reward: -200.0, Epsilon: 0.039\n",
      "Episode 646, Total Reward: -200.0, Epsilon: 0.039\n",
      "Episode 647, Total Reward: -200.0, Epsilon: 0.039\n",
      "Episode 648, Total Reward: -200.0, Epsilon: 0.039\n",
      "Episode 649, Total Reward: -200.0, Epsilon: 0.039\n",
      "Episode 650, Total Reward: -200.0, Epsilon: 0.038\n",
      "Episode 651, Total Reward: -200.0, Epsilon: 0.038\n",
      "Episode 652, Total Reward: -200.0, Epsilon: 0.038\n",
      "Episode 653, Total Reward: -200.0, Epsilon: 0.038\n",
      "Episode 654, Total Reward: -200.0, Epsilon: 0.038\n",
      "Episode 655, Total Reward: -200.0, Epsilon: 0.038\n",
      "Episode 656, Total Reward: -200.0, Epsilon: 0.037\n",
      "Episode 657, Total Reward: -200.0, Epsilon: 0.037\n",
      "Episode 658, Total Reward: -200.0, Epsilon: 0.037\n",
      "Episode 659, Total Reward: -200.0, Epsilon: 0.037\n",
      "Episode 660, Total Reward: -200.0, Epsilon: 0.037\n",
      "Episode 661, Total Reward: -200.0, Epsilon: 0.036\n",
      "Episode 662, Total Reward: -200.0, Epsilon: 0.036\n",
      "Episode 663, Total Reward: -200.0, Epsilon: 0.036\n",
      "Episode 664, Total Reward: -200.0, Epsilon: 0.036\n",
      "Episode 665, Total Reward: -200.0, Epsilon: 0.036\n",
      "Episode 666, Total Reward: -200.0, Epsilon: 0.035\n",
      "Episode 667, Total Reward: -200.0, Epsilon: 0.035\n",
      "Episode 668, Total Reward: -200.0, Epsilon: 0.035\n",
      "Episode 669, Total Reward: -200.0, Epsilon: 0.035\n",
      "Episode 670, Total Reward: -200.0, Epsilon: 0.035\n",
      "Episode 671, Total Reward: -200.0, Epsilon: 0.035\n",
      "Episode 672, Total Reward: -200.0, Epsilon: 0.034\n",
      "Episode 673, Total Reward: -200.0, Epsilon: 0.034\n",
      "Episode 674, Total Reward: -200.0, Epsilon: 0.034\n",
      "Episode 675, Total Reward: -200.0, Epsilon: 0.034\n",
      "Episode 676, Total Reward: -200.0, Epsilon: 0.034\n",
      "Episode 677, Total Reward: -200.0, Epsilon: 0.034\n",
      "Episode 678, Total Reward: -200.0, Epsilon: 0.033\n",
      "Episode 679, Total Reward: -200.0, Epsilon: 0.033\n",
      "Episode 680, Total Reward: -200.0, Epsilon: 0.033\n",
      "Episode 681, Total Reward: -200.0, Epsilon: 0.033\n",
      "Episode 682, Total Reward: -200.0, Epsilon: 0.033\n",
      "Episode 683, Total Reward: -200.0, Epsilon: 0.033\n",
      "Episode 684, Total Reward: -200.0, Epsilon: 0.032\n",
      "Episode 685, Total Reward: -200.0, Epsilon: 0.032\n",
      "Episode 686, Total Reward: -200.0, Epsilon: 0.032\n",
      "Episode 687, Total Reward: -200.0, Epsilon: 0.032\n",
      "Episode 688, Total Reward: -200.0, Epsilon: 0.032\n",
      "Episode 689, Total Reward: -200.0, Epsilon: 0.032\n",
      "Episode 690, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 691, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 692, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 693, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 694, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 695, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 696, Total Reward: -200.0, Epsilon: 0.031\n",
      "Episode 697, Total Reward: -200.0, Epsilon: 0.030\n",
      "Episode 698, Total Reward: -200.0, Epsilon: 0.030\n",
      "Episode 699, Total Reward: -200.0, Epsilon: 0.030\n",
      "Episode 700, Total Reward: -200.0, Epsilon: 0.030\n",
      "Episode 701, Total Reward: -200.0, Epsilon: 0.030\n",
      "Episode 702, Total Reward: -200.0, Epsilon: 0.030\n",
      "Episode 703, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 704, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 705, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 706, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 707, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 708, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 709, Total Reward: -200.0, Epsilon: 0.029\n",
      "Episode 710, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 711, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 712, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 713, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 714, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 715, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 716, Total Reward: -200.0, Epsilon: 0.028\n",
      "Episode 717, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 718, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 719, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 720, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 721, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 722, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 723, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 724, Total Reward: -200.0, Epsilon: 0.027\n",
      "Episode 725, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 726, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 727, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 728, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 729, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 730, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 731, Total Reward: -200.0, Epsilon: 0.026\n",
      "Episode 732, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 733, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 734, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 735, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 736, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 737, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 738, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 739, Total Reward: -200.0, Epsilon: 0.025\n",
      "Episode 740, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 741, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 742, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 743, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 744, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 745, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 746, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 747, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 748, Total Reward: -200.0, Epsilon: 0.024\n",
      "Episode 749, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 750, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 751, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 752, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 753, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 754, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 755, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 756, Total Reward: -200.0, Epsilon: 0.023\n",
      "Episode 757, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 758, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 759, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 760, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 761, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 762, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 763, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 764, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 765, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 766, Total Reward: -200.0, Epsilon: 0.022\n",
      "Episode 767, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 768, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 769, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 770, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 771, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 772, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 773, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 774, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 775, Total Reward: -200.0, Epsilon: 0.021\n",
      "Episode 776, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 777, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 778, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 779, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 780, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 781, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 782, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 783, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 784, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 785, Total Reward: -200.0, Epsilon: 0.020\n",
      "Episode 786, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 787, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 788, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 789, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 790, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 791, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 792, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 793, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 794, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 795, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 796, Total Reward: -200.0, Epsilon: 0.019\n",
      "Episode 797, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 798, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 799, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 800, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 801, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 802, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 803, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 804, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 805, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 806, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 807, Total Reward: -200.0, Epsilon: 0.018\n",
      "Episode 808, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 809, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 810, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 811, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 812, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 813, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 814, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 815, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 816, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 817, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 818, Total Reward: -200.0, Epsilon: 0.017\n",
      "Episode 819, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 820, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 821, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 822, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 823, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 824, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 825, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 826, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 827, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 828, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 829, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 830, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 831, Total Reward: -200.0, Epsilon: 0.016\n",
      "Episode 832, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 833, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 834, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 835, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 836, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 837, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 838, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 839, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 840, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 841, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 842, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 843, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 844, Total Reward: -200.0, Epsilon: 0.015\n",
      "Episode 845, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 846, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 847, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 848, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 849, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 850, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 851, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 852, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 853, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 854, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 855, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 856, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 857, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 858, Total Reward: -200.0, Epsilon: 0.014\n",
      "Episode 859, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 860, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 861, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 862, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 863, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 864, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 865, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 866, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 867, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 868, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 869, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 870, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 871, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 872, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 873, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 874, Total Reward: -200.0, Epsilon: 0.013\n",
      "Episode 875, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 876, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 877, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 878, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 879, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 880, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 881, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 882, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 883, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 884, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 885, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 886, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 887, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 888, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 889, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 890, Total Reward: -200.0, Epsilon: 0.012\n",
      "Episode 891, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 892, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 893, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 894, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 895, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 896, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 897, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 898, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 899, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 900, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 901, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 902, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 903, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 904, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 905, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 906, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 907, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 908, Total Reward: -200.0, Epsilon: 0.011\n",
      "Episode 909, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 910, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 911, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 912, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 913, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 914, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 915, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 916, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 917, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 918, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 919, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 920, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 921, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 922, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 923, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 924, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 925, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 926, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 927, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 928, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 929, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 930, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 931, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 932, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 933, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 934, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 935, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 936, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 937, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 938, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 939, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 940, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 941, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 942, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 943, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 944, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 945, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 946, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 947, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 948, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 949, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 950, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 951, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 952, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 953, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 954, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 955, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 956, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 957, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 958, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 959, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 960, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 961, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 962, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 963, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 964, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 965, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 966, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 967, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 968, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 969, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 970, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 971, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 972, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 973, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 974, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 975, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 976, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 977, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 978, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 979, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 980, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 981, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 982, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 983, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 984, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 985, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 986, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 987, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 988, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 989, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 990, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 991, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 992, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 993, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 994, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 995, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 996, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 997, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 998, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 999, Total Reward: -200.0, Epsilon: 0.010\n",
      "Episode 1000, Total Reward: -200.0, Epsilon: 0.010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0,\n",
       " -200.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 2, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 3, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 4, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 5, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 6, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 7, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 8, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 9, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 10, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 11, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 12, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 13, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 14, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 15, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 16, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 17, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 18, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 19, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 20, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 21, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 22, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 23, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 24, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 25, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 26, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 27, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 28, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 29, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 30, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 31, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 32, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 33, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 34, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 35, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 36, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 37, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 38, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 39, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 40, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 41, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 42, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 43, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 44, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 45, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 46, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 47, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 48, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 49, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 50, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 51, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 52, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 53, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 54, Total Reward: -200.0, Epsilon: 0.606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_STEPS):\n\u001b[0;32m---> 34\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     36\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[10], line 167\u001b[0m, in \u001b[0;36mDQNAgent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    165\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    166\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(state_tensor)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 167\u001b[0m certified_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_certified_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m certified_q \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Use the certified memory Q-value indirectly: choose best action from Q-network.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[10], line 85\u001b[0m, in \u001b[0;36mMemoryModule.get_certified_q\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     83\u001b[0m     mem_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatents[idx]\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Check if z is within the certified radius for this memory entry.\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmem_z\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mradii[idx]:\n\u001b[1;32m     86\u001b[0m         certified_qs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_values[idx])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(certified_qs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gymnasium/lib/python3.11/site-packages/numpy/linalg/linalg.py:2511\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2509\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[1;32m   2510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2511\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdot(x)\n\u001b[1;32m   2512\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Reward = 15.0, Epsilon = 0.995\n",
      "Episode 10: Reward = 41.0, Epsilon = 0.946\n",
      "Episode 20: Reward = 31.0, Epsilon = 0.900\n",
      "Episode 30: Reward = 18.0, Epsilon = 0.856\n",
      "Episode 40: Reward = 20.0, Epsilon = 0.814\n",
      "Episode 50: Reward = 14.0, Epsilon = 0.774\n",
      "Episode 60: Reward = 21.0, Epsilon = 0.737\n",
      "Episode 70: Reward = 22.0, Epsilon = 0.701\n",
      "Episode 80: Reward = 20.0, Epsilon = 0.666\n",
      "Episode 90: Reward = 27.0, Epsilon = 0.634\n",
      "Episode 100: Reward = 10.0, Epsilon = 0.603\n",
      "Episode 110: Reward = 16.0, Epsilon = 0.573\n",
      "Episode 120: Reward = 13.0, Epsilon = 0.545\n",
      "Episode 130: Reward = 12.0, Epsilon = 0.519\n",
      "Episode 140: Reward = 9.0, Epsilon = 0.493\n",
      "Episode 150: Reward = 12.0, Epsilon = 0.469\n",
      "Episode 160: Reward = 11.0, Epsilon = 0.446\n",
      "Episode 170: Reward = 10.0, Epsilon = 0.424\n",
      "Episode 180: Reward = 8.0, Epsilon = 0.404\n",
      "Episode 190: Reward = 9.0, Epsilon = 0.384\n",
      "Episode 200: Reward = 13.0, Epsilon = 0.365\n",
      "Episode 210: Reward = 9.0, Epsilon = 0.347\n",
      "Episode 220: Reward = 12.0, Epsilon = 0.330\n",
      "Episode 230: Reward = 13.0, Epsilon = 0.314\n",
      "Episode 240: Reward = 10.0, Epsilon = 0.299\n",
      "Episode 250: Reward = 12.0, Epsilon = 0.284\n",
      "Episode 260: Reward = 12.0, Epsilon = 0.270\n",
      "Episode 270: Reward = 10.0, Epsilon = 0.257\n",
      "Episode 280: Reward = 8.0, Epsilon = 0.245\n",
      "Episode 290: Reward = 9.0, Epsilon = 0.233\n",
      "Episode 300: Reward = 11.0, Epsilon = 0.221\n",
      "Episode 310: Reward = 10.0, Epsilon = 0.210\n",
      "Episode 320: Reward = 10.0, Epsilon = 0.200\n",
      "Episode 330: Reward = 11.0, Epsilon = 0.190\n",
      "Episode 340: Reward = 9.0, Epsilon = 0.181\n",
      "Episode 350: Reward = 12.0, Epsilon = 0.172\n",
      "Episode 360: Reward = 13.0, Epsilon = 0.164\n",
      "Episode 370: Reward = 16.0, Epsilon = 0.156\n",
      "Episode 380: Reward = 13.0, Epsilon = 0.148\n",
      "Episode 390: Reward = 38.0, Epsilon = 0.141\n",
      "Episode 400: Reward = 9.0, Epsilon = 0.134\n",
      "Episode 410: Reward = 13.0, Epsilon = 0.127\n",
      "Episode 420: Reward = 31.0, Epsilon = 0.121\n",
      "Episode 430: Reward = 70.0, Epsilon = 0.115\n",
      "Episode 440: Reward = 10.0, Epsilon = 0.110\n",
      "Episode 450: Reward = 12.0, Epsilon = 0.104\n",
      "Episode 460: Reward = 81.0, Epsilon = 0.099\n",
      "Episode 470: Reward = 42.0, Epsilon = 0.094\n",
      "Episode 480: Reward = 29.0, Epsilon = 0.090\n",
      "Episode 490: Reward = 32.0, Epsilon = 0.085\n",
      "Episode 500: Reward = 12.0, Epsilon = 0.081\n",
      "Episode 510: Reward = 70.0, Epsilon = 0.077\n",
      "Episode 520: Reward = 49.0, Epsilon = 0.073\n",
      "Episode 530: Reward = 87.0, Epsilon = 0.070\n",
      "Episode 540: Reward = 28.0, Epsilon = 0.066\n",
      "Episode 550: Reward = 49.0, Epsilon = 0.063\n",
      "Episode 560: Reward = 311.0, Epsilon = 0.060\n",
      "Episode 570: Reward = 169.0, Epsilon = 0.057\n",
      "Episode 580: Reward = 50.0, Epsilon = 0.054\n",
      "Episode 590: Reward = 86.0, Epsilon = 0.052\n",
      "Episode 600: Reward = 279.0, Epsilon = 0.050\n",
      "Episode 610: Reward = 95.0, Epsilon = 0.050\n",
      "Episode 620: Reward = 149.0, Epsilon = 0.050\n",
      "Episode 630: Reward = 112.0, Epsilon = 0.050\n",
      "Episode 640: Reward = 116.0, Epsilon = 0.050\n",
      "Episode 650: Reward = 141.0, Epsilon = 0.050\n",
      "Episode 660: Reward = 132.0, Epsilon = 0.050\n",
      "Episode 670: Reward = 196.0, Epsilon = 0.050\n",
      "Episode 680: Reward = 239.0, Epsilon = 0.050\n",
      "Episode 690: Reward = 222.0, Epsilon = 0.050\n",
      "Episode 700: Reward = 153.0, Epsilon = 0.050\n",
      "Episode 710: Reward = 120.0, Epsilon = 0.050\n",
      "Episode 720: Reward = 255.0, Epsilon = 0.050\n",
      "Episode 730: Reward = 169.0, Epsilon = 0.050\n",
      "Episode 740: Reward = 201.0, Epsilon = 0.050\n",
      "Episode 750: Reward = 199.0, Epsilon = 0.050\n",
      "Episode 760: Reward = 236.0, Epsilon = 0.050\n",
      "Episode 770: Reward = 282.0, Epsilon = 0.050\n",
      "Episode 780: Reward = 212.0, Epsilon = 0.050\n",
      "Episode 790: Reward = 393.0, Epsilon = 0.050\n",
      "Episode 800: Reward = 313.0, Epsilon = 0.050\n",
      "Episode 810: Reward = 15.0, Epsilon = 0.050\n",
      "Episode 820: Reward = 188.0, Epsilon = 0.050\n",
      "Episode 830: Reward = 196.0, Epsilon = 0.050\n",
      "Episode 840: Reward = 135.0, Epsilon = 0.050\n",
      "Episode 850: Reward = 362.0, Epsilon = 0.050\n",
      "Episode 860: Reward = 218.0, Epsilon = 0.050\n",
      "Episode 870: Reward = 252.0, Epsilon = 0.050\n",
      "Episode 880: Reward = 175.0, Epsilon = 0.050\n",
      "Episode 890: Reward = 205.0, Epsilon = 0.050\n",
      "Episode 900: Reward = 340.0, Epsilon = 0.050\n",
      "Episode 910: Reward = 172.0, Epsilon = 0.050\n",
      "Episode 920: Reward = 197.0, Epsilon = 0.050\n",
      "Episode 930: Reward = 199.0, Epsilon = 0.050\n",
      "Episode 940: Reward = 164.0, Epsilon = 0.050\n",
      "Episode 950: Reward = 173.0, Epsilon = 0.050\n",
      "Episode 960: Reward = 248.0, Epsilon = 0.050\n",
      "Episode 970: Reward = 269.0, Epsilon = 0.050\n",
      "Episode 980: Reward = 136.0, Epsilon = 0.050\n",
      "Episode 990: Reward = 237.0, Epsilon = 0.050\n",
      "Episode 1000: Reward = 104.0, Epsilon = 0.050\n",
      "Episode 1010: Reward = 62.0, Epsilon = 0.050\n",
      "Episode 1020: Reward = 82.0, Epsilon = 0.050\n",
      "Episode 1030: Reward = 125.0, Epsilon = 0.050\n",
      "Episode 1040: Reward = 189.0, Epsilon = 0.050\n",
      "Episode 1050: Reward = 175.0, Epsilon = 0.050\n",
      "Episode 1060: Reward = 137.0, Epsilon = 0.050\n",
      "Episode 1070: Reward = 340.0, Epsilon = 0.050\n",
      "Episode 1080: Reward = 183.0, Epsilon = 0.050\n",
      "Episode 1090: Reward = 147.0, Epsilon = 0.050\n",
      "Episode 1100: Reward = 173.0, Epsilon = 0.050\n",
      "Episode 1110: Reward = 139.0, Epsilon = 0.050\n",
      "Episode 1120: Reward = 140.0, Epsilon = 0.050\n",
      "Episode 1130: Reward = 200.0, Epsilon = 0.050\n",
      "Episode 1140: Reward = 408.0, Epsilon = 0.050\n",
      "Episode 1150: Reward = 333.0, Epsilon = 0.050\n",
      "Episode 1160: Reward = 500.0, Epsilon = 0.050\n",
      "Episode 1170: Reward = 239.0, Epsilon = 0.050\n",
      "Episode 1180: Reward = 338.0, Epsilon = 0.050\n",
      "Episode 1190: Reward = 370.0, Epsilon = 0.050\n",
      "Episode 1200: Reward = 244.0, Epsilon = 0.050\n",
      "Episode 1210: Reward = 245.0, Epsilon = 0.050\n",
      "Episode 1220: Reward = 264.0, Epsilon = 0.050\n",
      "Episode 1230: Reward = 263.0, Epsilon = 0.050\n",
      "Episode 1240: Reward = 157.0, Epsilon = 0.050\n",
      "Episode 1250: Reward = 261.0, Epsilon = 0.050\n",
      "Episode 1260: Reward = 282.0, Epsilon = 0.050\n",
      "Episode 1270: Reward = 214.0, Epsilon = 0.050\n",
      "Episode 1280: Reward = 214.0, Epsilon = 0.050\n",
      "Episode 1290: Reward = 166.0, Epsilon = 0.050\n",
      "Episode 1300: Reward = 138.0, Epsilon = 0.050\n",
      "Episode 1310: Reward = 146.0, Epsilon = 0.050\n",
      "Episode 1320: Reward = 396.0, Epsilon = 0.050\n",
      "Episode 1330: Reward = 367.0, Epsilon = 0.050\n",
      "Episode 1340: Reward = 182.0, Epsilon = 0.050\n",
      "Episode 1350: Reward = 126.0, Epsilon = 0.050\n",
      "Episode 1360: Reward = 18.0, Epsilon = 0.050\n",
      "Episode 1370: Reward = 115.0, Epsilon = 0.050\n",
      "Episode 1380: Reward = 15.0, Epsilon = 0.050\n",
      "Episode 1390: Reward = 17.0, Epsilon = 0.050\n",
      "Episode 1400: Reward = 98.0, Epsilon = 0.050\n",
      "Episode 1410: Reward = 23.0, Epsilon = 0.050\n",
      "Episode 1420: Reward = 125.0, Epsilon = 0.050\n",
      "Episode 1430: Reward = 122.0, Epsilon = 0.050\n",
      "Episode 1440: Reward = 17.0, Epsilon = 0.050\n",
      "Episode 1450: Reward = 122.0, Epsilon = 0.050\n",
      "Episode 1460: Reward = 159.0, Epsilon = 0.050\n",
      "Episode 1470: Reward = 135.0, Epsilon = 0.050\n",
      "Episode 1480: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 1490: Reward = 190.0, Epsilon = 0.050\n",
      "Episode 1500: Reward = 142.0, Epsilon = 0.050\n",
      "Episode 1510: Reward = 123.0, Epsilon = 0.050\n",
      "Episode 1520: Reward = 313.0, Epsilon = 0.050\n",
      "Episode 1530: Reward = 225.0, Epsilon = 0.050\n",
      "Episode 1540: Reward = 248.0, Epsilon = 0.050\n",
      "Episode 1550: Reward = 249.0, Epsilon = 0.050\n",
      "Episode 1560: Reward = 157.0, Epsilon = 0.050\n",
      "Episode 1570: Reward = 198.0, Epsilon = 0.050\n",
      "Episode 1580: Reward = 171.0, Epsilon = 0.050\n",
      "Episode 1590: Reward = 234.0, Epsilon = 0.050\n",
      "Episode 1600: Reward = 133.0, Epsilon = 0.050\n",
      "Episode 1610: Reward = 133.0, Epsilon = 0.050\n",
      "Episode 1620: Reward = 136.0, Epsilon = 0.050\n",
      "Episode 1630: Reward = 154.0, Epsilon = 0.050\n",
      "Episode 1640: Reward = 151.0, Epsilon = 0.050\n",
      "Episode 1650: Reward = 143.0, Epsilon = 0.050\n",
      "Episode 1660: Reward = 185.0, Epsilon = 0.050\n",
      "Episode 1670: Reward = 126.0, Epsilon = 0.050\n",
      "Episode 1680: Reward = 164.0, Epsilon = 0.050\n",
      "Episode 1690: Reward = 355.0, Epsilon = 0.050\n",
      "Episode 1700: Reward = 495.0, Epsilon = 0.050\n",
      "Episode 1710: Reward = 163.0, Epsilon = 0.050\n",
      "Episode 1720: Reward = 143.0, Epsilon = 0.050\n",
      "Episode 1730: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 1740: Reward = 283.0, Epsilon = 0.050\n",
      "Episode 1750: Reward = 177.0, Epsilon = 0.050\n",
      "Episode 1760: Reward = 195.0, Epsilon = 0.050\n",
      "Episode 1770: Reward = 137.0, Epsilon = 0.050\n",
      "Episode 1780: Reward = 167.0, Epsilon = 0.050\n",
      "Episode 1790: Reward = 164.0, Epsilon = 0.050\n",
      "Episode 1800: Reward = 140.0, Epsilon = 0.050\n",
      "Episode 1810: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 1820: Reward = 163.0, Epsilon = 0.050\n",
      "Episode 1830: Reward = 169.0, Epsilon = 0.050\n",
      "Episode 1840: Reward = 128.0, Epsilon = 0.050\n",
      "Episode 1850: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 1860: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 1870: Reward = 142.0, Epsilon = 0.050\n",
      "Episode 1880: Reward = 140.0, Epsilon = 0.050\n",
      "Episode 1890: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 1900: Reward = 20.0, Epsilon = 0.050\n",
      "Episode 1910: Reward = 187.0, Epsilon = 0.050\n",
      "Episode 1920: Reward = 96.0, Epsilon = 0.050\n",
      "Episode 1930: Reward = 110.0, Epsilon = 0.050\n",
      "Episode 1940: Reward = 132.0, Epsilon = 0.050\n",
      "Episode 1950: Reward = 119.0, Epsilon = 0.050\n",
      "Episode 1960: Reward = 162.0, Epsilon = 0.050\n",
      "Episode 1970: Reward = 16.0, Epsilon = 0.050\n",
      "Episode 1980: Reward = 11.0, Epsilon = 0.050\n",
      "Episode 1990: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 2000: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 2010: Reward = 9.0, Epsilon = 0.050\n",
      "Episode 2020: Reward = 87.0, Epsilon = 0.050\n",
      "Episode 2030: Reward = 9.0, Epsilon = 0.050\n",
      "Episode 2040: Reward = 123.0, Epsilon = 0.050\n",
      "Episode 2050: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 2060: Reward = 11.0, Epsilon = 0.050\n",
      "Episode 2070: Reward = 135.0, Epsilon = 0.050\n",
      "Episode 2080: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 2090: Reward = 135.0, Epsilon = 0.050\n",
      "Episode 2100: Reward = 16.0, Epsilon = 0.050\n",
      "Episode 2110: Reward = 132.0, Epsilon = 0.050\n",
      "Episode 2120: Reward = 116.0, Epsilon = 0.050\n",
      "Episode 2130: Reward = 18.0, Epsilon = 0.050\n",
      "Episode 2140: Reward = 134.0, Epsilon = 0.050\n",
      "Episode 2150: Reward = 179.0, Epsilon = 0.050\n",
      "Episode 2160: Reward = 99.0, Epsilon = 0.050\n",
      "Episode 2170: Reward = 136.0, Epsilon = 0.050\n",
      "Episode 2180: Reward = 20.0, Epsilon = 0.050\n",
      "Episode 2190: Reward = 105.0, Epsilon = 0.050\n",
      "Episode 2200: Reward = 24.0, Epsilon = 0.050\n",
      "Episode 2210: Reward = 111.0, Epsilon = 0.050\n",
      "Episode 2220: Reward = 109.0, Epsilon = 0.050\n",
      "Episode 2230: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 2240: Reward = 124.0, Epsilon = 0.050\n",
      "Episode 2250: Reward = 125.0, Epsilon = 0.050\n",
      "Episode 2260: Reward = 141.0, Epsilon = 0.050\n",
      "Episode 2270: Reward = 110.0, Epsilon = 0.050\n",
      "Episode 2280: Reward = 14.0, Epsilon = 0.050\n",
      "Episode 2290: Reward = 138.0, Epsilon = 0.050\n",
      "Episode 2300: Reward = 146.0, Epsilon = 0.050\n",
      "Episode 2310: Reward = 144.0, Epsilon = 0.050\n",
      "Episode 2320: Reward = 137.0, Epsilon = 0.050\n",
      "Episode 2330: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 2340: Reward = 122.0, Epsilon = 0.050\n",
      "Episode 2350: Reward = 142.0, Epsilon = 0.050\n",
      "Episode 2360: Reward = 146.0, Epsilon = 0.050\n",
      "Episode 2370: Reward = 129.0, Epsilon = 0.050\n",
      "Episode 2380: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 2390: Reward = 149.0, Epsilon = 0.050\n",
      "Episode 2400: Reward = 139.0, Epsilon = 0.050\n",
      "Episode 2410: Reward = 175.0, Epsilon = 0.050\n",
      "Episode 2420: Reward = 138.0, Epsilon = 0.050\n",
      "Episode 2430: Reward = 172.0, Epsilon = 0.050\n",
      "Episode 2440: Reward = 172.0, Epsilon = 0.050\n",
      "Episode 2450: Reward = 177.0, Epsilon = 0.050\n",
      "Episode 2460: Reward = 162.0, Epsilon = 0.050\n",
      "Episode 2470: Reward = 166.0, Epsilon = 0.050\n",
      "Episode 2480: Reward = 146.0, Epsilon = 0.050\n",
      "Episode 2490: Reward = 168.0, Epsilon = 0.050\n",
      "Episode 2500: Reward = 179.0, Epsilon = 0.050\n",
      "Episode 2510: Reward = 179.0, Epsilon = 0.050\n",
      "Episode 2520: Reward = 160.0, Epsilon = 0.050\n",
      "Episode 2530: Reward = 152.0, Epsilon = 0.050\n",
      "Episode 2540: Reward = 179.0, Epsilon = 0.050\n",
      "Episode 2550: Reward = 164.0, Epsilon = 0.050\n",
      "Episode 2560: Reward = 145.0, Epsilon = 0.050\n",
      "Episode 2570: Reward = 154.0, Epsilon = 0.050\n",
      "Episode 2580: Reward = 200.0, Epsilon = 0.050\n",
      "Episode 2590: Reward = 140.0, Epsilon = 0.050\n",
      "Episode 2600: Reward = 142.0, Epsilon = 0.050\n",
      "Episode 2610: Reward = 185.0, Epsilon = 0.050\n",
      "Episode 2620: Reward = 220.0, Epsilon = 0.050\n",
      "Episode 2630: Reward = 127.0, Epsilon = 0.050\n",
      "Episode 2640: Reward = 262.0, Epsilon = 0.050\n",
      "Episode 2650: Reward = 150.0, Epsilon = 0.050\n",
      "Episode 2660: Reward = 133.0, Epsilon = 0.050\n",
      "Episode 2670: Reward = 162.0, Epsilon = 0.050\n",
      "Episode 2680: Reward = 182.0, Epsilon = 0.050\n",
      "Episode 2690: Reward = 124.0, Epsilon = 0.050\n",
      "Episode 2700: Reward = 125.0, Epsilon = 0.050\n",
      "Episode 2710: Reward = 246.0, Epsilon = 0.050\n",
      "Episode 2720: Reward = 115.0, Epsilon = 0.050\n",
      "Episode 2730: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 2740: Reward = 139.0, Epsilon = 0.050\n",
      "Episode 2750: Reward = 127.0, Epsilon = 0.050\n",
      "Episode 2760: Reward = 112.0, Epsilon = 0.050\n",
      "Episode 2770: Reward = 148.0, Epsilon = 0.050\n",
      "Episode 2780: Reward = 145.0, Epsilon = 0.050\n",
      "Episode 2790: Reward = 135.0, Epsilon = 0.050\n",
      "Episode 2800: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 2810: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 2820: Reward = 14.0, Epsilon = 0.050\n",
      "Episode 2830: Reward = 184.0, Epsilon = 0.050\n",
      "Episode 2840: Reward = 106.0, Epsilon = 0.050\n",
      "Episode 2850: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 2860: Reward = 104.0, Epsilon = 0.050\n",
      "Episode 2870: Reward = 169.0, Epsilon = 0.050\n",
      "Episode 2880: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 2890: Reward = 124.0, Epsilon = 0.050\n",
      "Episode 2900: Reward = 94.0, Epsilon = 0.050\n",
      "Episode 2910: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 2920: Reward = 94.0, Epsilon = 0.050\n",
      "Episode 2930: Reward = 162.0, Epsilon = 0.050\n",
      "Episode 2940: Reward = 194.0, Epsilon = 0.050\n",
      "Episode 2950: Reward = 313.0, Epsilon = 0.050\n",
      "Episode 2960: Reward = 166.0, Epsilon = 0.050\n",
      "Episode 2970: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 2980: Reward = 79.0, Epsilon = 0.050\n",
      "Episode 2990: Reward = 132.0, Epsilon = 0.050\n",
      "Episode 3000: Reward = 154.0, Epsilon = 0.050\n",
      "Episode 3010: Reward = 107.0, Epsilon = 0.050\n",
      "Episode 3020: Reward = 75.0, Epsilon = 0.050\n",
      "Episode 3030: Reward = 129.0, Epsilon = 0.050\n",
      "Episode 3040: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 3050: Reward = 112.0, Epsilon = 0.050\n",
      "Episode 3060: Reward = 67.0, Epsilon = 0.050\n",
      "Episode 3070: Reward = 85.0, Epsilon = 0.050\n",
      "Episode 3080: Reward = 75.0, Epsilon = 0.050\n",
      "Episode 3090: Reward = 105.0, Epsilon = 0.050\n",
      "Episode 3100: Reward = 103.0, Epsilon = 0.050\n",
      "Episode 3110: Reward = 107.0, Epsilon = 0.050\n",
      "Episode 3120: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 3130: Reward = 101.0, Epsilon = 0.050\n",
      "Episode 3140: Reward = 112.0, Epsilon = 0.050\n",
      "Episode 3150: Reward = 89.0, Epsilon = 0.050\n",
      "Episode 3160: Reward = 107.0, Epsilon = 0.050\n",
      "Episode 3170: Reward = 198.0, Epsilon = 0.050\n",
      "Episode 3180: Reward = 91.0, Epsilon = 0.050\n",
      "Episode 3190: Reward = 118.0, Epsilon = 0.050\n",
      "Episode 3200: Reward = 100.0, Epsilon = 0.050\n",
      "Episode 3210: Reward = 174.0, Epsilon = 0.050\n",
      "Episode 3220: Reward = 431.0, Epsilon = 0.050\n",
      "Episode 3230: Reward = 108.0, Epsilon = 0.050\n",
      "Episode 3240: Reward = 212.0, Epsilon = 0.050\n",
      "Episode 3250: Reward = 128.0, Epsilon = 0.050\n",
      "Episode 3260: Reward = 109.0, Epsilon = 0.050\n",
      "Episode 3270: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 3280: Reward = 118.0, Epsilon = 0.050\n",
      "Episode 3290: Reward = 120.0, Epsilon = 0.050\n",
      "Episode 3300: Reward = 120.0, Epsilon = 0.050\n",
      "Episode 3310: Reward = 119.0, Epsilon = 0.050\n",
      "Episode 3320: Reward = 116.0, Epsilon = 0.050\n",
      "Episode 3330: Reward = 131.0, Epsilon = 0.050\n",
      "Episode 3340: Reward = 106.0, Epsilon = 0.050\n",
      "Episode 3350: Reward = 115.0, Epsilon = 0.050\n",
      "Episode 3360: Reward = 127.0, Epsilon = 0.050\n",
      "Episode 3370: Reward = 151.0, Epsilon = 0.050\n",
      "Episode 3380: Reward = 178.0, Epsilon = 0.050\n",
      "Episode 3390: Reward = 122.0, Epsilon = 0.050\n",
      "Episode 3400: Reward = 143.0, Epsilon = 0.050\n",
      "Episode 3410: Reward = 150.0, Epsilon = 0.050\n",
      "Episode 3420: Reward = 173.0, Epsilon = 0.050\n",
      "Episode 3430: Reward = 283.0, Epsilon = 0.050\n",
      "Episode 3440: Reward = 124.0, Epsilon = 0.050\n",
      "Episode 3450: Reward = 147.0, Epsilon = 0.050\n",
      "Episode 3460: Reward = 154.0, Epsilon = 0.050\n",
      "Episode 3470: Reward = 319.0, Epsilon = 0.050\n",
      "Episode 3480: Reward = 157.0, Epsilon = 0.050\n",
      "Episode 3490: Reward = 183.0, Epsilon = 0.050\n",
      "Episode 3500: Reward = 208.0, Epsilon = 0.050\n",
      "Episode 3510: Reward = 162.0, Epsilon = 0.050\n",
      "Episode 3520: Reward = 126.0, Epsilon = 0.050\n",
      "Episode 3530: Reward = 181.0, Epsilon = 0.050\n",
      "Episode 3540: Reward = 140.0, Epsilon = 0.050\n",
      "Episode 3550: Reward = 158.0, Epsilon = 0.050\n",
      "Episode 3560: Reward = 157.0, Epsilon = 0.050\n",
      "Episode 3570: Reward = 260.0, Epsilon = 0.050\n",
      "Episode 3580: Reward = 106.0, Epsilon = 0.050\n",
      "Episode 3590: Reward = 179.0, Epsilon = 0.050\n",
      "Episode 3600: Reward = 18.0, Epsilon = 0.050\n",
      "Episode 3610: Reward = 178.0, Epsilon = 0.050\n",
      "Episode 3620: Reward = 11.0, Epsilon = 0.050\n",
      "Episode 3630: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 3640: Reward = 11.0, Epsilon = 0.050\n",
      "Episode 3650: Reward = 100.0, Epsilon = 0.050\n",
      "Episode 3660: Reward = 9.0, Epsilon = 0.050\n",
      "Episode 3670: Reward = 109.0, Epsilon = 0.050\n",
      "Episode 3680: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 3690: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 3700: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 3710: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 3720: Reward = 11.0, Epsilon = 0.050\n",
      "Episode 3730: Reward = 136.0, Epsilon = 0.050\n",
      "Episode 3740: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 3750: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 3760: Reward = 8.0, Epsilon = 0.050\n",
      "Episode 3770: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 3780: Reward = 79.0, Epsilon = 0.050\n",
      "Episode 3790: Reward = 14.0, Epsilon = 0.050\n",
      "Episode 3800: Reward = 147.0, Epsilon = 0.050\n",
      "Episode 3810: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 3820: Reward = 107.0, Epsilon = 0.050\n",
      "Episode 3830: Reward = 125.0, Epsilon = 0.050\n",
      "Episode 3840: Reward = 224.0, Epsilon = 0.050\n",
      "Episode 3850: Reward = 96.0, Epsilon = 0.050\n",
      "Episode 3860: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 3870: Reward = 377.0, Epsilon = 0.050\n",
      "Episode 3880: Reward = 113.0, Epsilon = 0.050\n",
      "Episode 3890: Reward = 110.0, Epsilon = 0.050\n",
      "Episode 3900: Reward = 105.0, Epsilon = 0.050\n",
      "Episode 3910: Reward = 94.0, Epsilon = 0.050\n",
      "Episode 3920: Reward = 105.0, Epsilon = 0.050\n",
      "Episode 3930: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 3940: Reward = 127.0, Epsilon = 0.050\n",
      "Episode 3950: Reward = 96.0, Epsilon = 0.050\n",
      "Episode 3960: Reward = 115.0, Epsilon = 0.050\n",
      "Episode 3970: Reward = 16.0, Epsilon = 0.050\n",
      "Episode 3980: Reward = 153.0, Epsilon = 0.050\n",
      "Episode 3990: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 4000: Reward = 16.0, Epsilon = 0.050\n",
      "Episode 4010: Reward = 149.0, Epsilon = 0.050\n",
      "Episode 4020: Reward = 126.0, Epsilon = 0.050\n",
      "Episode 4030: Reward = 123.0, Epsilon = 0.050\n",
      "Episode 4040: Reward = 205.0, Epsilon = 0.050\n",
      "Episode 4050: Reward = 189.0, Epsilon = 0.050\n",
      "Episode 4060: Reward = 131.0, Epsilon = 0.050\n",
      "Episode 4070: Reward = 199.0, Epsilon = 0.050\n",
      "Episode 4080: Reward = 244.0, Epsilon = 0.050\n",
      "Episode 4090: Reward = 208.0, Epsilon = 0.050\n",
      "Episode 4100: Reward = 171.0, Epsilon = 0.050\n",
      "Episode 4110: Reward = 149.0, Epsilon = 0.050\n",
      "Episode 4120: Reward = 238.0, Epsilon = 0.050\n",
      "Episode 4130: Reward = 243.0, Epsilon = 0.050\n",
      "Episode 4140: Reward = 227.0, Epsilon = 0.050\n",
      "Episode 4150: Reward = 143.0, Epsilon = 0.050\n",
      "Episode 4160: Reward = 169.0, Epsilon = 0.050\n",
      "Episode 4170: Reward = 217.0, Epsilon = 0.050\n",
      "Episode 4180: Reward = 200.0, Epsilon = 0.050\n",
      "Episode 4190: Reward = 189.0, Epsilon = 0.050\n",
      "Episode 4200: Reward = 130.0, Epsilon = 0.050\n",
      "Episode 4210: Reward = 181.0, Epsilon = 0.050\n",
      "Episode 4220: Reward = 132.0, Epsilon = 0.050\n",
      "Episode 4230: Reward = 158.0, Epsilon = 0.050\n",
      "Episode 4240: Reward = 201.0, Epsilon = 0.050\n",
      "Episode 4250: Reward = 139.0, Epsilon = 0.050\n",
      "Episode 4260: Reward = 145.0, Epsilon = 0.050\n",
      "Episode 4270: Reward = 124.0, Epsilon = 0.050\n",
      "Episode 4280: Reward = 17.0, Epsilon = 0.050\n",
      "Episode 4290: Reward = 426.0, Epsilon = 0.050\n",
      "Episode 4300: Reward = 147.0, Epsilon = 0.050\n",
      "Episode 4310: Reward = 181.0, Epsilon = 0.050\n",
      "Episode 4320: Reward = 154.0, Epsilon = 0.050\n",
      "Episode 4330: Reward = 110.0, Epsilon = 0.050\n",
      "Episode 4340: Reward = 108.0, Epsilon = 0.050\n",
      "Episode 4350: Reward = 268.0, Epsilon = 0.050\n",
      "Episode 4360: Reward = 166.0, Epsilon = 0.050\n",
      "Episode 4370: Reward = 88.0, Epsilon = 0.050\n",
      "Episode 4380: Reward = 143.0, Epsilon = 0.050\n",
      "Episode 4390: Reward = 156.0, Epsilon = 0.050\n",
      "Episode 4400: Reward = 500.0, Epsilon = 0.050\n",
      "Episode 4410: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 4420: Reward = 259.0, Epsilon = 0.050\n",
      "Episode 4430: Reward = 173.0, Epsilon = 0.050\n",
      "Episode 4440: Reward = 106.0, Epsilon = 0.050\n",
      "Episode 4450: Reward = 102.0, Epsilon = 0.050\n",
      "Episode 4460: Reward = 104.0, Epsilon = 0.050\n",
      "Episode 4470: Reward = 102.0, Epsilon = 0.050\n",
      "Episode 4480: Reward = 111.0, Epsilon = 0.050\n",
      "Episode 4490: Reward = 104.0, Epsilon = 0.050\n",
      "Episode 4500: Reward = 109.0, Epsilon = 0.050\n",
      "Episode 4510: Reward = 100.0, Epsilon = 0.050\n",
      "Episode 4520: Reward = 92.0, Epsilon = 0.050\n",
      "Episode 4530: Reward = 77.0, Epsilon = 0.050\n",
      "Episode 4540: Reward = 84.0, Epsilon = 0.050\n",
      "Episode 4550: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 4560: Reward = 82.0, Epsilon = 0.050\n",
      "Episode 4570: Reward = 94.0, Epsilon = 0.050\n",
      "Episode 4580: Reward = 101.0, Epsilon = 0.050\n",
      "Episode 4590: Reward = 117.0, Epsilon = 0.050\n",
      "Episode 4600: Reward = 138.0, Epsilon = 0.050\n",
      "Episode 4610: Reward = 330.0, Epsilon = 0.050\n",
      "Episode 4620: Reward = 102.0, Epsilon = 0.050\n",
      "Episode 4630: Reward = 87.0, Epsilon = 0.050\n",
      "Episode 4640: Reward = 164.0, Epsilon = 0.050\n",
      "Episode 4650: Reward = 215.0, Epsilon = 0.050\n",
      "Episode 4660: Reward = 106.0, Epsilon = 0.050\n",
      "Episode 4670: Reward = 121.0, Epsilon = 0.050\n",
      "Episode 4680: Reward = 115.0, Epsilon = 0.050\n",
      "Episode 4690: Reward = 109.0, Epsilon = 0.050\n",
      "Episode 4700: Reward = 106.0, Epsilon = 0.050\n",
      "Episode 4710: Reward = 107.0, Epsilon = 0.050\n",
      "Episode 4720: Reward = 105.0, Epsilon = 0.050\n",
      "Episode 4730: Reward = 88.0, Epsilon = 0.050\n",
      "Episode 4740: Reward = 108.0, Epsilon = 0.050\n",
      "Episode 4750: Reward = 94.0, Epsilon = 0.050\n",
      "Episode 4760: Reward = 95.0, Epsilon = 0.050\n",
      "Episode 4770: Reward = 45.0, Epsilon = 0.050\n",
      "Episode 4780: Reward = 79.0, Epsilon = 0.050\n",
      "Episode 4790: Reward = 113.0, Epsilon = 0.050\n",
      "Episode 4800: Reward = 88.0, Epsilon = 0.050\n",
      "Episode 4810: Reward = 103.0, Epsilon = 0.050\n",
      "Episode 4820: Reward = 224.0, Epsilon = 0.050\n",
      "Episode 4830: Reward = 110.0, Epsilon = 0.050\n",
      "Episode 4840: Reward = 134.0, Epsilon = 0.050\n",
      "Episode 4850: Reward = 120.0, Epsilon = 0.050\n",
      "Episode 4860: Reward = 126.0, Epsilon = 0.050\n",
      "Episode 4870: Reward = 123.0, Epsilon = 0.050\n",
      "Episode 4880: Reward = 177.0, Epsilon = 0.050\n",
      "Episode 4890: Reward = 113.0, Epsilon = 0.050\n",
      "Episode 4900: Reward = 454.0, Epsilon = 0.050\n",
      "Episode 4910: Reward = 17.0, Epsilon = 0.050\n",
      "Episode 4920: Reward = 12.0, Epsilon = 0.050\n",
      "Episode 4930: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 4940: Reward = 114.0, Epsilon = 0.050\n",
      "Episode 4950: Reward = 167.0, Epsilon = 0.050\n",
      "Episode 4960: Reward = 12.0, Epsilon = 0.050\n",
      "Episode 4970: Reward = 10.0, Epsilon = 0.050\n",
      "Episode 4980: Reward = 13.0, Epsilon = 0.050\n",
      "Episode 4990: Reward = 23.0, Epsilon = 0.050\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.05\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "CERTIFIED_SAMPLES = 10  # Number of samples for certified region estimation\n",
    "\n",
    "# Define Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Certified Region Estimation\n",
    "def compute_certified_radii_batch(q_network, states, actions):\n",
    "    N = states.shape[0]\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(states)  # (N, num_actions)\n",
    "        q_selected = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute certified bounds (artificially simulate confidence bounds)\n",
    "        scales = torch.rand((N, CERTIFIED_SAMPLES), device=device) * q_selected.unsqueeze(1)\n",
    "        radii = torch.mean(scales, dim=1)  # Average certified region\n",
    "    return radii\n",
    "\n",
    "# Define the Agent\n",
    "class CertifiedRegionAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.steps = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            max_next_q_values = torch.max(next_q_values, dim=1)[0]\n",
    "            targets = rewards + (1 - dones) * GAMMA * max_next_q_values\n",
    "\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        loss = nn.MSELoss()(q_values, targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Certified Region Update\n",
    "        certified_radii = compute_certified_radii_batch(self.q_network, states, actions)\n",
    "        adjustment_factor = torch.mean(certified_radii).item()\n",
    "        \n",
    "        # Adjust learning rate based on certified bounds\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = max(LEARNING_RATE * adjustment_factor, 1e-5)\n",
    "\n",
    "        # Update target network periodically\n",
    "        if self.steps % 100 == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon * EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "# Training Function\n",
    "def train():\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    agent = CertifiedRegionAgent(state_dim, action_dim)\n",
    "\n",
    "    num_episodes = 5000\n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        agent.update()\n",
    "        agent.decay_epsilon()\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: Reward = {total_reward}, Epsilon = {agent.epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return rewards_history\n",
    "\n",
    "# Run Training\n",
    "if __name__ == \"__main__\":\n",
    "    scores = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
